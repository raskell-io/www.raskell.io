<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
      <title>Raskell</title>
      <link>https://raskell.io</link>
      <description>Writing about platform automation, edge systems, applied security, and open standards. Building automation-first platforms that survive production reality.</description>
      <generator>Zola</generator>
      <language>en</language>
      <atom:link href="https://raskell.io/rss.xml" rel="self" type="application/rss+xml"/>
      <lastBuildDate>Wed, 11 Feb 2026 00:00:00 +0000</lastBuildDate>
      <item>
          <title>Edge Systems Are the New Backend</title>
          <pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate>
          <author>Unknown</author>
          <link>https://raskell.io/articles/edge-systems-are-the-new-backend/</link>
          <guid>https://raskell.io/articles/edge-systems-are-the-new-backend/</guid>
          <description xml:base="https://raskell.io/articles/edge-systems-are-the-new-backend/">&lt;p&gt;A request arrives at your system. In the next 50 milliseconds, before any application code runs, this happens: TLS termination, route matching, WAF inspection against 285 detection rules, JWT validation, rate limit evaluation, request body validation against a JSON schema, and trace context generation. The request either dies at the edge or arrives at your backend pre-authenticated, pre-validated, and pre-authorized.&lt;&#x2F;p&gt;
&lt;p&gt;Five years ago, your backend did all of this. Every service validated its own tokens, enforced its own rate limits, ran its own security checks. Today, the backend might not even exist in the form you expect. It might be a static site served from edge nodes, a thin persistence API, or a headless CMS that publishes content to a CDN and never handles a user request directly.&lt;&#x2F;p&gt;
&lt;p&gt;Something shifted. Not just at the edge. On both ends.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-three-tier-past&quot;&gt;The three-tier past&lt;&#x2F;h2&gt;
&lt;p&gt;The architecture most of us learned was simple. Client, backend, database. The browser rendered HTML, maybe ran some jQuery. The backend did everything: authentication, authorization, business logic, rendering, validation, rate limiting, session management. The database stored state. Clean separation, one direction, easy to reason about.&lt;&#x2F;p&gt;
&lt;p&gt;This model worked because the browser was dumb. It could render markup and submit forms. Any real computation had to happen on the server. The backend was fat by necessity, not by design.&lt;&#x2F;p&gt;
&lt;p&gt;Microservices made it worse. Consider a typical setup: a user service, an order service, a payment service, a notification service, an inventory service. Each one needs to validate JWTs. Each one needs to enforce rate limits. Each one needs input validation, request logging, and error handling. That is five services times six concerns. Thirty implementations of logic that should exist exactly once.&lt;&#x2F;p&gt;
&lt;p&gt;Now multiply. Real organizations have 15, 50, 200 services. Each team implements auth slightly differently. One uses a shared library, one copied the code two years ago, one rolled their own because the library did not support their token format. The rate limiting configurations drift. The logging formats diverge. A security patch to the JWT validation logic means PRs across every repository, coordinated deployments, and someone asking “did we get all of them?”&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;                 ┌──────────┬──────────┬──────────┐
                 │ Users    │ Orders   │ Payments │
                 │ Service  │ Service  │ Service  │
                 ├──────────┼──────────┼──────────┤
  Auth           │ ✓ (v2.1) │ ✓ (v1.9) │ ✓ (v2.0)│
  Rate limiting  │ ✓ (lib)  │ ✓ (copy) │ ✗ (none)│
  Validation     │ ✓        │ ✓        │ ✓       │
  WAF&amp;#x2F;Security   │ ✗        │ ✗        │ ✗       │
  Logging        │ JSON     │ text     │ JSON    │
  Tracing        │ ✓        │ ✗        │ ✓       │
                 └──────────┴──────────┴──────────┘
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Libraries helped. Service meshes helped more. But the complexity was still distributed across every service, in every team’s codebase, in every deployment pipeline. The mesh moved networking concerns to a sidecar. It did not move application-level concerns like auth, validation, or security inspection.&lt;&#x2F;p&gt;
&lt;p&gt;The edge was an afterthought. A reverse proxy. TLS termination. Maybe Varnish for caching. Maybe a CDN for static assets. It was infrastructure plumbing, not a place where decisions happened.&lt;&#x2F;p&gt;
&lt;p&gt;That model is over.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;two-migrations-one-hollowing&quot;&gt;Two migrations, one hollowing&lt;&#x2F;h2&gt;
&lt;p&gt;Here is the thing I keep coming back to: business logic is migrating in two directions simultaneously.&lt;&#x2F;p&gt;
&lt;p&gt;Upward, to the edge. Infrastructure concerns like auth, WAF, and rate limiting now execute at the edge layer, before requests reach any backend. But it goes further than that. Edge Workers run actual application code. Containers deploy at the edge. Server-side rendering happens at edge nodes 50ms from the user, not in a data center 200ms away.&lt;&#x2F;p&gt;
&lt;p&gt;Downward, to the client. The browser is no longer dumb. WebAssembly runs near-native code. WebGPU puts the GPU to work on ML inference and image processing. Web Workers handle background computation. Service Workers intercept network requests and serve cached responses offline. CRDTs let the client own its data and sync when it feels like it.&lt;&#x2F;p&gt;
&lt;p&gt;The backend is caught in the middle. Squeezed from both sides. And what remains is not a “backend” in any traditional sense. It is a persistence layer. A place where data rests and syncs. The interesting work happens elsewhere.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-moved-to-the-edge&quot;&gt;What moved to the edge&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;infrastructure-concerns&quot;&gt;Infrastructure concerns&lt;&#x2F;h3&gt;
&lt;p&gt;The first wave was obvious. Cross-cutting concerns that every service needed are better handled once, at the point of entry.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Authentication.&lt;&#x2F;strong&gt; Validating a JWT does not require application context. The token is self-contained: a signature, an issuer, an expiry, a set of claims. Parse it, verify the signature against a JWKS endpoint, check the expiry, extract the claims, attach them as headers. Done. The backend receives &lt;code&gt;X-User-Id: alice&lt;&#x2F;code&gt; and &lt;code&gt;X-User-Role: admin&lt;&#x2F;code&gt; instead of a raw Bearer token it has to decode itself.&lt;&#x2F;p&gt;
&lt;p&gt;This is not hypothetical. Here is what this looks like in practice:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;kdl&quot; class=&quot;language-kdl &quot;&gt;&lt;code class=&quot;language-kdl&quot; data-lang=&quot;kdl&quot;&gt;agent &amp;quot;auth&amp;quot; {
    type &amp;quot;auth&amp;quot;
    grpc address=&amp;quot;http:&amp;#x2F;&amp;#x2F;localhost:50051&amp;quot;
    events &amp;quot;request_headers&amp;quot;
    timeout-ms 100
    failure-mode &amp;quot;closed&amp;quot;
    max-concurrent-calls 100
}
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;That agent handles JWT, OIDC, SAML, mTLS, and API key validation. Every route behind it gets authentication for free. Every backend service trusts the edge to have done the work. The auth agent crashes? Failure mode is “closed”. Requests stop, but the proxy stays up.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Rate limiting.&lt;&#x2F;strong&gt; Token bucket algorithms with per-client keys. The edge layer sees every request before the backend does. It is the natural place to enforce rate limits because it can reject bad traffic before it consumes backend resources. A rejected request at the edge costs microseconds. A rejected request at the backend costs a database query, a connection slot, and whatever work happened before the check.&lt;&#x2F;p&gt;
&lt;p&gt;There are two flavors. Local rate limiting uses in-process token buckets. Fast, no network hops, but each edge node tracks its own counters. If you have 10 edge nodes and a limit of 100 requests per second, each node allows 100, so the effective limit is 1,000. For most use cases, this is fine. Abuse does not distribute itself evenly across your infrastructure.&lt;&#x2F;p&gt;
&lt;p&gt;Distributed rate limiting uses a shared store (Redis, typically). Accurate across nodes, but adds a network hop per request. The tradeoff is latency versus precision. I default to local rate limiting and switch to distributed only when the use case demands exact global limits, like API billing or token budgets for LLM inference.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Security inspection.&lt;&#x2F;strong&gt; WAFs used to be appliances. Expensive, opaque, binary. A request was either blocked or allowed. Modern WAFs use anomaly scoring. Each rule contributes a score, and the total determines the action:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;Score 0-9:    Allow
Score 10-24:  Log (warning, investigate later)
Score 25+:    Block
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This is a fundamentally different model than binary block&#x2F;allow. It lets you tune aggressively without breaking legitimate traffic. I run 285 detection rules at the edge and process 912K requests per second on clean traffic. That is 30x faster than ModSecurity’s C implementation. The performance gap matters because it means WAF inspection can happen on every request, not just suspicious ones.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;API validation.&lt;&#x2F;strong&gt; If your API has a JSON Schema, why validate request bodies in your application code? Validate at the edge. Reject malformed requests before they consume a connection, a goroutine, a database transaction. The backend receives only structurally valid payloads.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Observability.&lt;&#x2F;strong&gt; Trace context should originate at the edge, not at the application. The edge is where the request enters your system. It is where you assign a trace ID, start the clock, and record the first span. If you originate traces in your application, you miss everything that happened before: TLS negotiation time, WAF processing time, the fact that the request sat in a rate limit queue for 50ms. Starting traces at the edge gives you the full picture.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;the-isolation-problem&quot;&gt;The isolation problem&lt;&#x2F;h3&gt;
&lt;p&gt;You cannot put all of this in a monolithic proxy. That is how you end up with nginx and 47 modules where nobody understands the interaction effects. A WAF bug should not take down your routing. A slow auth provider should not block rate limit checks.&lt;&#x2F;p&gt;
&lt;p&gt;The answer is process isolation. Thin dataplane, crash-isolated external agents. Each agent runs as a separate process with its own failure domain:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;┌──────────────────────────────────────────┐
│ Edge Proxy (thin dataplane)              │
│ Routing │ TLS │ Caching │ Load Balancing │
└─────┬──────────┬──────────┬──────────────┘
      │          │          │
      ▼          ▼          ▼
   [WAF]      [Auth]    [Rate Limit]
  process     process     process
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Each agent gets its own concurrency semaphore. A slow WAF cannot starve auth. Each agent has a circuit breaker. Three failures in 30 seconds and the circuit opens. Each agent has a configurable failure mode, and this is where the design gets interesting:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;kdl&quot; class=&quot;language-kdl &quot;&gt;&lt;code class=&quot;language-kdl&quot; data-lang=&quot;kdl&quot;&gt;agent &amp;quot;waf&amp;quot; {
    type &amp;quot;waf&amp;quot;
    timeout-ms 100
    failure-mode &amp;quot;closed&amp;quot;
    max-concurrent-calls 50
    circuit-breaker {
        failure-threshold 5
        success-threshold 2
        timeout-seconds 30
    }
}

agent &amp;quot;rate-limit&amp;quot; {
    type &amp;quot;rate-limit&amp;quot;
    timeout-ms 50
    failure-mode &amp;quot;open&amp;quot;
    max-concurrent-calls 200
}
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The WAF fails closed. If it crashes or times out, requests are blocked. You lose availability to preserve security. Rate limiting fails open. If it crashes, requests are allowed. You lose rate enforcement to preserve availability. These are explicit choices per agent, not global defaults. The operator decides which tradeoff to make for each concern, and the decision is visible in the config, not buried in code.&lt;&#x2F;p&gt;
&lt;p&gt;Agents return decisions. The proxy merges them. A blocking decision from any agent wins. Otherwise, header mutations accumulate. The model is simple: agents advise, the proxy decides. No agent can override another agent’s block. No agent can force a request through. The proxy owns the final call.&lt;&#x2F;p&gt;
&lt;p&gt;This is not a workaround. It is the fundamental design choice. Complex logic lives outside the core, behind process boundaries. The proxy stays small, fast, and boring. The agents handle the interesting work in isolation. A bug in a Lua scripting agent does not corrupt the routing table. A memory leak in the WAF agent does not exhaust the proxy’s memory. The process boundary is the blast radius.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;edge-workers-business-logic-at-the-edge&quot;&gt;Edge Workers: business logic at the edge&lt;&#x2F;h3&gt;
&lt;p&gt;Infrastructure concerns were the first wave. The second wave is actual business logic.&lt;&#x2F;p&gt;
&lt;p&gt;Cloudflare Workers, Deno Deploy, Fastly Compute, Vercel Edge Functions. These are not just “serverless at the CDN.” They are full compute environments running at edge nodes around the world. V8 isolates spin up in under 5ms. Cold starts are measured in single-digit milliseconds, not seconds. Your code runs 50ms from the user instead of 200ms away in us-east-1.&lt;&#x2F;p&gt;
&lt;p&gt;The constraints matter, because they shape what belongs here. Typical Edge Worker limits: 10-50ms CPU time per request (not wall time, actual CPU), 128MB memory, no raw TCP sockets, no persistent file system. You get a request, key-value storage, and the ability to make sub-requests to origins. That is it. These constraints are not bugs. They are what makes sub-millisecond cold starts possible. V8 isolates are cheap because they are small and short-lived.&lt;&#x2F;p&gt;
&lt;p&gt;What fits within these constraints is surprisingly broad:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;API routing and transformation.&lt;&#x2F;strong&gt; A request comes in for &lt;code&gt;&#x2F;api&#x2F;v2&#x2F;users&lt;&#x2F;code&gt;. The edge Worker rewrites it, fans out to two backend services (user profiles from one, preferences from another), merges the responses, and returns a single payload. The backend services are simple data sources. The edge Worker is the API layer.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;A&#x2F;B testing and feature flags.&lt;&#x2F;strong&gt; Read the experiment cookie, hash the user ID, assign a variant, route to the right origin or rewrite the response. No round trip to a feature flag service. The decision happens in microseconds at the node closest to the user.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Personalization.&lt;&#x2F;strong&gt; Look up the user’s segment in KV storage, inject the right content block, set cache headers accordingly. The backend generated all variants at build time. The edge picks the right one per request.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Server-side rendering.&lt;&#x2F;strong&gt; Render HTML at the edge node closest to the user. Frameworks like Next.js and Remix already support this. React Server Components run at the edge. The “server” in server-side rendering is not your server. It is an edge node in 300 locations.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Authentication and session management.&lt;&#x2F;strong&gt; Validate tokens, refresh sessions, set secure cookies. The auth flow never touches your origin. Cloudflare Workers KV or Durable Objects store session state at the edge.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The pattern: compute that depends on request context but not on deep application state moves to the edge. If you can do it with a request, a key-value lookup, and a response, it probably belongs here. If it needs a complex database query or a multi-step transaction, it does not.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;containers-at-the-edge&quot;&gt;Containers at the edge&lt;&#x2F;h3&gt;
&lt;p&gt;Edge Workers hit a ceiling when you need persistent connections, large memory, or long-running processes. For those workloads, containers at the edge.&lt;&#x2F;p&gt;
&lt;p&gt;Fly.io, Railway, and Lambda@Edge deploy containers or full processes to edge locations worldwide. Your application runs with real file systems, TCP connections, and whatever runtime you need. But it runs close to users, not in a centralized data center. Latency drops from 200ms to 20ms.&lt;&#x2F;p&gt;
&lt;p&gt;The interesting problem is data gravity. Compute is easy to distribute. Data is not. If your container runs in Tokyo but your database is in Frankfurt, you have not solved the latency problem. You have moved it from the user-to-server hop to the server-to-database hop. The solutions are still maturing: read replicas at the edge (Turso, Neon), embedded databases that sync (LiteFS, libSQL), and eventually-consistent stores designed for multi-region (DynamoDB Global Tables, CockroachDB).&lt;&#x2F;p&gt;
&lt;p&gt;This model makes sense when compute and data can be co-located:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Regional APIs&lt;&#x2F;strong&gt; that comply with data residency requirements. Run the container and the database replica in the same region. GDPR data stays in the EU. Japanese user data stays in Japan.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Real-time applications&lt;&#x2F;strong&gt; where 200ms round trips kill the experience. Collaborative editing, multiplayer, live dashboards. A WebSocket server 20ms away feels instant. One 200ms away feels sluggish.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Stateful edge compute&lt;&#x2F;strong&gt; where you need more than a request&#x2F;response cycle. Background processing, scheduled jobs, long-running connections.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The line between “edge” and “origin” blurs. If your container runs in 30 regions and handles requests locally with a local database replica, is that an edge deployment or a distributed backend? The distinction stops mattering. What matters is that the compute and the data are close to the user.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-moved-to-the-client&quot;&gt;What moved to the client&lt;&#x2F;h2&gt;
&lt;p&gt;The other half of the migration goes downward. The browser is not the thin client it used to be.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;webassembly&quot;&gt;WebAssembly&lt;&#x2F;h3&gt;
&lt;p&gt;WASM runs at near-native speed in every modern browser. Not “fast for JavaScript.” Actually fast. Compiled from Rust, C++, Go, or any language with an LLVM backend. Sandboxed, portable, deterministic.&lt;&#x2F;p&gt;
&lt;p&gt;What this enables:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Image and video processing&lt;&#x2F;strong&gt; in the browser. No upload to a server, no round trip, no privacy concern. The pixels never leave the device.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Document parsing and transformation.&lt;&#x2F;strong&gt; PDF rendering, spreadsheet computation, file format conversion. Libraries compiled to WASM and running client-side.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Cryptographic operations.&lt;&#x2F;strong&gt; End-to-end encryption where the server never sees plaintext. Key derivation, signing, verification, all in the browser.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Full relational databases in the browser.&lt;&#x2F;strong&gt; This is the one that changes architectures.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;SQLite compiled to WASM (via projects like sql.js, wa-sqlite, or the official SQLite WASM build) gives the browser a real relational database. Not a key-value store. Not IndexedDB’s awkward object store API. Actual SQL with joins, indexes, transactions, and triggers. Backed by the Origin Private File System (OPFS) for persistence, it survives page reloads and browser restarts.&lt;&#x2F;p&gt;
&lt;p&gt;The implications are significant. Your application can run complex queries locally. Filter, sort, aggregate, full-text search. All instant, all offline. The server becomes a sync endpoint. It ships a database snapshot down and accepts change sets back. The client does the querying. The server does the storing.&lt;&#x2F;p&gt;
&lt;p&gt;This pattern scales down elegantly. A note-taking app with SQLite-in-WASM needs no backend API for reads. A project management tool can filter and search 10,000 tasks without a network request. A CMS authoring interface can work fully offline and sync when the author reconnects. The read path is local. The write path syncs eventually.&lt;&#x2F;p&gt;
&lt;p&gt;WASI (WebAssembly System Interface) extends this further. It gives WASM modules controlled access to file systems, clocks, and network sockets outside the browser. WASM becomes a universal runtime: the same binary runs in the browser, at the edge (Cloudflare Workers use WASM under the hood), and on bare metal. Write once, deploy to every layer of the stack.&lt;&#x2F;p&gt;
&lt;p&gt;The pattern: anything that is CPU-bound, privacy-sensitive, or latency-sensitive is a candidate for client-side WASM. If the computation does not need server-side state, it should not round-trip to a server.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;webgpu&quot;&gt;WebGPU&lt;&#x2F;h3&gt;
&lt;p&gt;WebGPU landed in Chrome in 2023, in Firefox and Safari shortly after, and it changes the math on what the client can compute. This is not WebGL with a new name. WebGL exposes a graphics pipeline. WebGPU exposes compute shaders. Direct, general-purpose GPU computation from JavaScript or WASM.&lt;&#x2F;p&gt;
&lt;p&gt;The immediate application is ML inference. Run a language model, an image classifier, or a recommendation engine on the user’s GPU. No server call, no API cost per token, no latency. The model weights download once (cached by the browser) and run locally. Privacy by default, because the data never leaves the device.&lt;&#x2F;p&gt;
&lt;p&gt;This is not theoretical. Stable Diffusion generates images in the browser via WebGPU. Small language models (Phi-2, Gemma 2B, Llama 3.2 1B) run at usable speeds on consumer hardware. MediaPipe runs pose detection, face tracking, and hand gesture recognition in real time. The trajectory is clear: models get smaller through distillation and quantization, consumer GPUs get faster, and the gap between “cloud inference” and “local inference” narrows every quarter.&lt;&#x2F;p&gt;
&lt;p&gt;But inference is not the only use case. WebGPU handles any parallel computation: physics simulations for games, signal processing for audio applications, particle systems for data visualization, and large-scale matrix operations. Anything you would reach for CUDA or Metal for on native can now run in the browser. The compute budget of the client just increased by orders of magnitude.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;web-workers-and-service-workers&quot;&gt;Web Workers and Service Workers&lt;&#x2F;h3&gt;
&lt;p&gt;Web Workers give you background threads. Heavy computation does not block the UI. Parse a large file, run a simulation, index a search corpus. All off the main thread, all without janking the interface.&lt;&#x2F;p&gt;
&lt;p&gt;Service Workers sit between the browser and the network. They intercept every fetch request and decide what to do: serve from cache, go to network, do both and race them. This enables:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Offline-first applications.&lt;&#x2F;strong&gt; The app works without a network connection. Data syncs when connectivity returns.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Background sync.&lt;&#x2F;strong&gt; Queue mutations while offline, replay them when online.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Push notifications.&lt;&#x2F;strong&gt; Wake the app without the user having it open.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Intelligent caching.&lt;&#x2F;strong&gt; Cache API responses, serve stale data while revalidating, pre-fetch resources the user is likely to need.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The Service Worker is the client-side equivalent of the edge proxy. It intercepts, caches, validates, and routes. It makes the client self-sufficient.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;local-first-and-crdts&quot;&gt;Local-first and CRDTs&lt;&#x2F;h3&gt;
&lt;p&gt;Here is where it gets interesting. If the client has compute (WASM, WebGPU, Web Workers) and storage (IndexedDB, OPFS) and offline capability (Service Workers), why does it need a server at all?&lt;&#x2F;p&gt;
&lt;p&gt;CRDTs (Conflict-free Replicated Data Types) answer the consistency question. Multiple clients can edit the same data independently, offline, with no coordination. When they reconnect, their changes merge automatically without conflicts. No server-mediated locking. No “last write wins” data loss. Mathematical guarantees that concurrent edits converge to the same state.&lt;&#x2F;p&gt;
&lt;p&gt;The architecture:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;Client A (offline)     Client B (offline)
    │                      │
    ├── Local edits        ├── Local edits
    │   (CRDT ops)         │   (CRDT ops)
    │                      │
    └──────┐      ┌────────┘
           ▼      ▼
      ┌──────────────┐
      │ Sync service  │  (thin, stateless)
      │ (persistence  │
      │  + relay)     │
      └──────────────┘
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The sync service is not a backend. It stores operations and relays them between clients. It does not run business logic. It does not validate (the CRDT handles consistency). It does not transform (the merge function is built into the data type). It is a persistence layer with a WebSocket attached.&lt;&#x2F;p&gt;
&lt;p&gt;I build systems like this. The concrete model: a document is a flat &lt;code&gt;HashMap&amp;lt;EntityId, Entity&amp;gt;&lt;&#x2F;code&gt; where each entity holds CRDT-typed fields. The field types determine how concurrent edits merge:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;CRDT type&lt;&#x2F;th&gt;&lt;th&gt;Merge behavior&lt;&#x2F;th&gt;&lt;th&gt;Use case&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;LwwRegister&amp;lt;T&amp;gt;&lt;&#x2F;td&gt;&lt;td&gt;Last writer wins (by timestamp)&lt;&#x2F;td&gt;&lt;td&gt;Simple values: name, status, URL&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;GrowOnlySet&amp;lt;T&amp;gt;&lt;&#x2F;td&gt;&lt;td&gt;Union of both sides&lt;&#x2F;td&gt;&lt;td&gt;Tags, labels, immutable references&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;ObservedRemoveSet&amp;lt;T&amp;gt;&lt;&#x2F;td&gt;&lt;td&gt;Add wins over concurrent remove&lt;&#x2F;td&gt;&lt;td&gt;Collaborator lists, mutable collections&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;MaxRegister&lt;&#x2F;td&gt;&lt;td&gt;Higher value wins&lt;&#x2F;td&gt;&lt;td&gt;Version counters, progress indicators&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;MinRegister&lt;&#x2F;td&gt;&lt;td&gt;Lower value wins&lt;&#x2F;td&gt;&lt;td&gt;Earliest timestamps, priority values&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;Each field carries a hybrid logical clock (HLC) timestamp. The HLC combines physical time with a logical counter, so causality is preserved even when wall clocks drift. Two clients edit the same field at the “same” time? The HLC ordering is deterministic. Both clients converge to the same value without coordination.&lt;&#x2F;p&gt;
&lt;p&gt;The merge function has three properties that make this work: it is associative (grouping does not matter), commutative (order does not matter), and idempotent (applying the same operation twice has no additional effect). These are not implementation details. They are the mathematical foundation that makes server-free consistency possible. You can sync operations in any order, from any number of clients, through any number of intermediate relays, and every replica converges to the same state.&lt;&#x2F;p&gt;
&lt;p&gt;The client owns its data. The server is optional. When the server exists, it persists operations and relays them. It does not arbitrate, transform, or validate beyond authentication.&lt;&#x2F;p&gt;
&lt;p&gt;This is not a niche pattern for collaborative text editors. Any application where users create and modify data can benefit. Notes, task managers, project planning tools, CMS authoring, form builders. The question is not “should this be local-first?” The question is “does this need a server, and if so, for what?”&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-the-backend-becomes&quot;&gt;What the backend becomes&lt;&#x2F;h2&gt;
&lt;p&gt;If the edge handles infrastructure concerns and business logic that depends on request context, and the client handles computation, rendering, and local state, what is left for the backend?&lt;&#x2F;p&gt;
&lt;p&gt;A persistence layer.&lt;&#x2F;p&gt;
&lt;p&gt;The backend becomes the place where data rests between sessions and syncs between devices. Not an application server. A persistence layer.&lt;&#x2F;p&gt;
&lt;p&gt;Consider the spectrum of what “backend” looks like now:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Static sites.&lt;&#x2F;strong&gt; This site is an example. raskell.io is built with Zola. Markdown files compile to HTML at build time and deploy to edge CDN nodes. No application server. No database. No runtime process. The “backend” is a git repository and a CI pipeline. Content lives as files. Serving happens at the edge. The total monthly infrastructure cost is the price of a domain name.&lt;&#x2F;p&gt;
&lt;p&gt;This is not limited to blogs. Documentation sites, marketing pages, product landing pages, e-commerce storefronts with pre-rendered product pages. Any content that changes at author-time rather than request-time can be static. The headless CMS (Contentful, Sanity, Strapi, or just a git repo) publishes content. The static site generator builds HTML. The CDN serves it. The “backend” runs at build time, not at request time.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Thin persistence APIs.&lt;&#x2F;strong&gt; For applications with dynamic data, the backend shrinks to a database with an API in front of it. Accept writes, serve reads, enforce schema constraints. GraphQL or REST over Postgres. No rendering. No business logic beyond data integrity. The API exists so that clients and edge workers have somewhere to store and retrieve state.&lt;&#x2F;p&gt;
&lt;p&gt;The interesting shift: even the persistence API is getting thinner. Services like Supabase, PlanetScale, and Turso expose the database directly over HTTP or WebSockets with built-in auth. Your “backend” becomes a hosted database with row-level security policies. No application code at all.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Sync relays.&lt;&#x2F;strong&gt; For local-first applications, the backend is even simpler. Accept CRDT operations from clients, persist them to durable storage, fan them out to other connected clients via WebSocket. No merge logic (the CRDT handles that). No transformation. No validation beyond authentication. The relay does not understand the data. It stores and forwards.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Event logs.&lt;&#x2F;strong&gt; Append-only storage. Clients sync by replaying events from their last known position. The log is the source of truth. Everything else (search indexes, analytics dashboards, recommendation models) is a materialized view built asynchronously. The hot path is the append. The read path is the replay. Both are simple.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Batch processors.&lt;&#x2F;strong&gt; The one place where traditional backend compute survives: jobs that require access to the full dataset. Analytics aggregation, report generation, search index building, ML model training. These run on schedules or triggers, not in the request path. They read from the event log or the database, compute, and write results back. The user never waits for them.&lt;&#x2F;p&gt;
&lt;p&gt;The common thread: the backend does not touch the hot path. User requests hit the edge and the client. The backend runs in the background, on its own schedule, when no one is waiting.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-architecture-that-makes-this-work&quot;&gt;The architecture that makes this work&lt;&#x2F;h2&gt;
&lt;p&gt;Pushing logic to the edge and the client is not free. Both environments have constraints, and ignoring them is how you build fragile systems.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;at-the-edge-bounded-resources&quot;&gt;At the edge: bounded resources&lt;&#x2F;h3&gt;
&lt;p&gt;Every operation at the edge needs explicit limits. No open-ended computations, no unbounded queues, no surprise behavior. This is not just good practice. It is existential. The edge proxy sits between the internet and your infrastructure. If it behaves unpredictably, everything behind it suffers.&lt;&#x2F;p&gt;
&lt;p&gt;Concretely:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Resource&lt;&#x2F;th&gt;&lt;th&gt;Bound&lt;&#x2F;th&gt;&lt;th&gt;Why&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;Agent concurrency&lt;&#x2F;td&gt;&lt;td&gt;Per-agent semaphore (default: 100)&lt;&#x2F;td&gt;&lt;td&gt;Prevents noisy neighbor between agents&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Agent timeout&lt;&#x2F;td&gt;&lt;td&gt;100ms default&lt;&#x2F;td&gt;&lt;td&gt;Prevents latency cascade&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Connection pool&lt;&#x2F;td&gt;&lt;td&gt;Explicit max (default: 10K)&lt;&#x2F;td&gt;&lt;td&gt;Prevents file descriptor exhaustion&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Request body&lt;&#x2F;td&gt;&lt;td&gt;Streaming, not buffered&lt;&#x2F;td&gt;&lt;td&gt;Prevents memory exhaustion&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Route cache&lt;&#x2F;td&gt;&lt;td&gt;LRU with size limit&lt;&#x2F;td&gt;&lt;td&gt;Prevents unbounded growth&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Rate limit queues&lt;&#x2F;td&gt;&lt;td&gt;Bounded with max delay&lt;&#x2F;td&gt;&lt;td&gt;Prevents request pile-up&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;If you cannot articulate the bound for every resource your edge system uses, you do not have an architecture. You have an accident waiting for load.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;on-the-client-isolation-and-sandboxing&quot;&gt;On the client: isolation and sandboxing&lt;&#x2F;h3&gt;
&lt;p&gt;The client has different constraints. Battery life, memory pressure, the user closing the tab at any moment.&lt;&#x2F;p&gt;
&lt;p&gt;WASM runs in a sandbox. No file system access, no network access, no shared memory (unless explicitly granted). This is the security model that makes client-side compute viable. Untrusted code (your own, running on someone else’s device) cannot escape the sandbox.&lt;&#x2F;p&gt;
&lt;p&gt;Web Workers run in separate threads with message-passing. No shared mutable state. No locks. No data races. The isolation is enforced by the runtime, not by programmer discipline.&lt;&#x2F;p&gt;
&lt;p&gt;Service Workers have a lifecycle managed by the browser. They can be terminated at any time to save resources. Your offline logic must handle graceful shutdown. This means: durable state in IndexedDB, idempotent sync operations, no in-memory state that cannot be reconstructed.&lt;&#x2F;p&gt;
&lt;p&gt;CRDTs provide consistency guarantees without coordination. But they are not magic. They consume memory (tombstones for deleted items, version vectors for causal ordering). They need garbage collection. They need careful schema design because not every data model maps cleanly to CRDT primitives. A counter works. A last-writer-wins register works. A rich text document with formatting, comments, and embedded media requires careful thought.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;the-trust-boundary&quot;&gt;The trust boundary&lt;&#x2F;h3&gt;
&lt;p&gt;Here is the part most edge-computing articles skip: trust.&lt;&#x2F;p&gt;
&lt;p&gt;If the edge handles auth, the backend trusts the edge to have done auth correctly. If the client handles business logic, the server trusts the client to have computed correctly. These are real trust boundaries with real failure modes.&lt;&#x2F;p&gt;
&lt;p&gt;At the edge, trust is earned through:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Failure isolation.&lt;&#x2F;strong&gt; Agent crashes do not take down the proxy. Bad config is validated before activation.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Observability.&lt;&#x2F;strong&gt; Every decision is logged, metered, and traceable. If the WAF blocked a request, you can see exactly which rules fired and why.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Bounded behavior.&lt;&#x2F;strong&gt; No surprise modes. Every resource has explicit limits. Every failure mode is configured, not assumed.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;On the client, trust is conditional:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Never trust the client for security decisions.&lt;&#x2F;strong&gt; Validate at the edge or the backend. Client-side checks are UX, not security.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Trust the client for its own data.&lt;&#x2F;strong&gt; If the user is editing their own document, the client is authoritative. CRDTs handle consistency. The server persists, it does not arbitrate.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Verify at the boundary.&lt;&#x2F;strong&gt; When client data syncs to the server, validate schema and authorization. Trust the merge, verify the input.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;when-not-to-do-this&quot;&gt;When not to do this&lt;&#x2F;h2&gt;
&lt;p&gt;Not everything belongs at the edge or on the client. Here is what stays in the backend:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Multi-service transactions.&lt;&#x2F;strong&gt; If an operation needs to read from three databases, check inventory, charge a payment, and send a notification, that is a backend workflow. Distributed transactions need coordination, and coordination needs a central authority.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Heavy data joins.&lt;&#x2F;strong&gt; If your query joins six tables with complex filters and aggregations, it runs next to the database, not at an edge node 200ms away from the data.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Regulatory requirements.&lt;&#x2F;strong&gt; Some industries mandate that data processing happens in specific locations, on specific infrastructure, with specific audit trails. Edge deployment may not satisfy these constraints.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Small teams with simple needs.&lt;&#x2F;strong&gt; If you have one backend, ten users, and no latency problems, this architecture is overhead. A Django app behind nginx is fine. Optimize when you have a reason to optimize, not before.&lt;&#x2F;p&gt;
&lt;p&gt;The edge handles cross-cutting concerns and request-context computation. The client handles local state and user-facing compute. The backend handles coordination, persistence, and anything that needs the full dataset. Know which is which.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;where-this-is-going&quot;&gt;Where this is going&lt;&#x2F;h2&gt;
&lt;p&gt;Five years ago, the stack was: browser (thin) renders server-generated HTML, backend (fat) runs everything, database stores state. The mental model was request&#x2F;response, and the backend was the center of gravity.&lt;&#x2F;p&gt;
&lt;p&gt;The stack now:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;┌─────────────────────────────────────────────────────┐
│ Client                                               │
│ WASM │ WebGPU │ Web Workers │ Service Workers │ CRDT │
│ (compute, render, offline, local state)              │
└────────────────────┬────────────────────────────────┘
                     │
┌────────────────────┴────────────────────────────────┐
│ Edge                                                 │
│ Proxy │ Workers │ Containers │ KV │ Durable Objects  │
│ (auth, WAF, routing, SSR, API aggregation, policy)   │
└────────────────────┬────────────────────────────────┘
                     │
┌────────────────────┴────────────────────────────────┐
│ Backend                                              │
│ Database │ Sync relay │ Event log │ Batch processing  │
│ (persistence, coordination, async compute)           │
└─────────────────────────────────────────────────────┘
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The client is fat. The edge is fat. The backend is thin. The center of gravity moved to both ends simultaneously.&lt;&#x2F;p&gt;
&lt;p&gt;Every year, this accelerates. Models get smaller and run on consumer GPUs. WASM runtimes get faster and gain more system APIs through WASI. Edge platforms add durable storage, queues, and cron triggers. CRDTs mature from academic curiosities to production libraries. SQLite-in-the-browser goes from experiment to default architecture for offline-capable apps.&lt;&#x2F;p&gt;
&lt;p&gt;The backend will not disappear. Data needs to live somewhere durable, and cross-device sync needs a relay. Coordination problems need a central authority. Batch processing needs access to the full dataset. But the backend’s role is narrowing to exactly these things. It is becoming infrastructure, not application. Plumbing, not logic.&lt;&#x2F;p&gt;
&lt;p&gt;I find myself building systems where the most interesting engineering happens at the boundaries. A reverse proxy that inspects 912K requests per second through 285 WAF rules, authenticates with sub-millisecond latency, and routes with crash-isolated agents. A client that owns its data through CRDTs, syncs when it feels like it, and runs inference on the local GPU. Between them, a database. Necessary and boring.&lt;&#x2F;p&gt;
&lt;p&gt;The backend is not dead. It is just not where the interesting work happens anymore.&lt;&#x2F;p&gt;
</description>
      </item>
      <item>
          <title>Looking back on 2025</title>
          <pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate>
          <author>Unknown</author>
          <link>https://raskell.io/articles/looking-back-on-2025/</link>
          <guid>https://raskell.io/articles/looking-back-on-2025/</guid>
          <description xml:base="https://raskell.io/articles/looking-back-on-2025/">&lt;p&gt;I spent part of this year on the shores of Okinawa. The water there is something else entirely — this impossible azure that shifts to turquoise in the shallows, so clear you can see the coral formations from the surface. I found myself thinking about systems while I was there, the way you do when you’re floating in salt water with nothing pressing to attend to.&lt;&#x2F;p&gt;
&lt;p&gt;Between swims, I read Tim Berners-Lee’s “This is for everyone.” I’ve been building web software for over a decade now, and I thought I understood what the web was. But reading TBL’s words while watching that reef ecosystem do its thing — thousands of species in constant exchange, no central coordinator, just emergent complexity from simple rules — something shifted in how I saw it all.&lt;&#x2F;p&gt;
&lt;p&gt;The web TBL imagined was supposed to work like that reef. A commons. Many small nodes, each doing their own thing, connected through open protocols. Information flowing freely. The beauty of it wasn’t in any single node but in the connections between them, the way the whole became more than the sum of its parts. The same principle that makes a reef resilient makes a network powerful: diversity, redundancy, local adaptation.&lt;&#x2F;p&gt;
&lt;p&gt;What we built instead looks more like industrial aquaculture. Five platforms. Algorithmic monoculture. Content optimized for engagement metrics rather than usefulness. We took a system designed for decentralization and built the most centralized information infrastructure in human history.&lt;&#x2F;p&gt;
&lt;p&gt;I keep thinking about how that happened. The web itself never changed — HTTP still works the same way, HTML still does what it always did. What changed was the economics. Publishing became free, but being &lt;em&gt;found&lt;&#x2F;em&gt; became expensive. The platforms positioned themselves as the gatekeepers of attention, and suddenly you couldn’t reach people without paying the toll, whether in ad spend or in algorithmic compliance or in the slow erosion of doing whatever it took to game SEO.&lt;&#x2F;p&gt;
&lt;p&gt;The thing about monocultures is they’re efficient right up until they’re not. A reef can lose a species and adapt. A monoculture gets one disease and collapses. We’ve been watching the web’s monoculture show stress fractures for years — the enshittification of platforms, the SEO content farms drowning out signal with noise, the way social media stopped being social and started being a feed of engagement-optimized content from strangers.&lt;&#x2F;p&gt;
&lt;p&gt;Then 2025 happened, and AI started breaking things in interesting ways.&lt;&#x2F;p&gt;
&lt;p&gt;The obvious take is that AI makes the content problem worse. And superficially, that’s true — if you thought SEO spam was bad before, wait until you see what happens when generating ten thousand pages of plausible-sounding garbage costs essentially nothing. The content farms went into overdrive. Social platforms filled with synthetic engagement.&lt;&#x2F;p&gt;
&lt;p&gt;But here’s the thing I keep coming back to: maybe that’s the fever that breaks the infection.&lt;&#x2F;p&gt;
&lt;p&gt;The old economics of the web depended on a particular scarcity. Human attention is finite, and the platforms controlled access to it. You wanted eyeballs, you played their game. SEO worked because Google was the gateway and you could optimize for what Google wanted. Platform distribution mattered because that’s where the people were.&lt;&#x2F;p&gt;
&lt;p&gt;AI disrupts this in ways that I think are genuinely interesting. When an AI assistant can synthesize information from across the web and deliver it directly to the user, the value of ranking first on Google diminishes. Why click through to a content farm when the answer is already in front of you? When AI agents can find and surface relevant content directly, you don’t need to be on the platform where the eyeballs gather. The middleman’s leverage starts to evaporate.&lt;&#x2F;p&gt;
&lt;p&gt;And crucially: when everyone can generate infinite content at zero marginal cost, content quantity becomes worthless. What matters is provenance. Accuracy. Usefulness. The things that are actually hard. The things that require a human perspective, or at least require &lt;em&gt;being right&lt;&#x2F;em&gt; in ways that matter.&lt;&#x2F;p&gt;
&lt;p&gt;I find myself unexpectedly optimistic about what comes next.&lt;&#x2F;p&gt;
&lt;p&gt;If AI breaks the distribution stranglehold that platforms have, the economics of the web could flip in interesting directions. The old model needed scale because reaching people was expensive. But if AI handles discovery — finding relevant content and bringing it to users — then maybe you don’t need scale anymore. Maybe small becomes viable again.&lt;&#x2F;p&gt;
&lt;p&gt;Think about what this means concretely. A static site costs nearly nothing to run. No databases to scale, no servers to babysit, just files sitting on edge nodes around the world. If you don’t need to capture user data for ad-driven personalization, you don’t need the complexity of the surveillance stack. If you don’t need platform distribution, you don’t need to play platform games.&lt;&#x2F;p&gt;
&lt;p&gt;There’s another piece to this that I think most people are missing: edge computing changes what personalization can mean. The conventional wisdom is that personalization requires surveillance — you need to know everything about a user to show them relevant content. But that’s only true if personalization happens in a centralized database somewhere. If personalization happens at the edge, at the moment of request, you can adapt content to context without ever needing to know who the user is. The edge function doesn’t need a profile. It just needs to know what was asked for and what context it’s being asked in.&lt;&#x2F;p&gt;
&lt;p&gt;This is the architecture I keep thinking about: static content at the origin, edge functions that adapt it anonymously, AI agents that find and surface it based on actual relevance rather than SEO gaming. No surveillance required. No platform dependency. No scaling costs that force you into growth-at-all-costs mode.&lt;&#x2F;p&gt;
&lt;p&gt;It looks more like a reef than a fish farm.&lt;&#x2F;p&gt;
&lt;p&gt;I don’t want to oversell this. The transition, if it happens, won’t be clean. The platforms aren’t going to quietly cede control. The incentives that built the current web are still operating. And AI itself could go in directions that make things worse rather than better — there are plenty of dystopian paths from here.&lt;&#x2F;p&gt;
&lt;p&gt;But when I think about what I want to build toward, it’s that reef model. Many small, specialized nodes. Interconnected through open protocols. Resilient because distributed. Sustainable because the economics work at small scale.&lt;&#x2F;p&gt;
&lt;p&gt;This site is part of that bet. Static content, no tracking, no platform dependencies. The tools I’m working on — Sentinel, Sango, Ushio — they’re all about making edge infrastructure more accessible, making it easier to build and operate systems that are distributed and independent.&lt;&#x2F;p&gt;
&lt;p&gt;2025 was the year AI started breaking the old model. I don’t know exactly what grows in its place. But floating in that Okinawan water, watching the reef do what reefs do, I got a sense of what healthy systems look like. Diverse. Interconnected. Resilient. Not optimized for any single metric, but somehow working anyway.&lt;&#x2F;p&gt;
&lt;p&gt;That’s what I’m betting on.&lt;&#x2F;p&gt;
</description>
      </item>
      <item>
          <title>Mise ate my Makefile</title>
          <pubDate>Sun, 14 Dec 2025 00:00:00 +0000</pubDate>
          <author>Unknown</author>
          <link>https://raskell.io/articles/mise-ate-my-makefile/</link>
          <guid>https://raskell.io/articles/mise-ate-my-makefile/</guid>
          <description xml:base="https://raskell.io/articles/mise-ate-my-makefile/">&lt;h2 id=&quot;the-problem-with-project-setup&quot;&gt;The problem with project setup&lt;&#x2F;h2&gt;
&lt;p&gt;Every project starts the same. You need Ruby 3.2.1 but have 2.7. You need Node 20 but have 18. Someone wrote a Makefile that assumes GNU make but you’re on BSD. The &lt;code&gt;scripts&#x2F;&lt;&#x2F;code&gt; folder has 47 shell scripts and nobody remembers what half of them do.&lt;&#x2F;p&gt;
&lt;p&gt;I found mise. It fixed all of this.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-mise-actually-is&quot;&gt;What mise actually is&lt;&#x2F;h2&gt;
&lt;p&gt;mise started as a Rust rewrite of asdf. Then it absorbed make’s job too. Now it’s the one tool I install on every machine.&lt;&#x2F;p&gt;
&lt;p&gt;Here’s what my typical project setup looked like before:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;.ruby-version&lt;&#x2F;code&gt; for rbenv&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;.nvmrc&lt;&#x2F;code&gt; for node&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;Makefile&lt;&#x2F;code&gt; with 20 targets&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;scripts&#x2F;&lt;&#x2F;code&gt; with random shell scripts&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;.env.example&lt;&#x2F;code&gt; that nobody updates&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Here’s what it looks like now:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;toml&quot; class=&quot;language-toml &quot;&gt;&lt;code class=&quot;language-toml&quot; data-lang=&quot;toml&quot;&gt;# .mise.toml
[tools]
ruby = &amp;quot;3.2.1&amp;quot;
node = &amp;quot;20.11.0&amp;quot;
python = &amp;quot;3.12&amp;quot;

[tasks.test]
run = &amp;quot;bundle exec rspec &amp;amp;&amp;amp; npm test&amp;quot;
description = &amp;quot;Run all tests&amp;quot;

[tasks.deploy]
run = &amp;quot;kubectl apply -f k8s&amp;#x2F;&amp;quot;
depends = [&amp;quot;test&amp;quot;, &amp;quot;build&amp;quot;]

[env]
DATABASE_URL = &amp;quot;postgresql:&amp;#x2F;&amp;#x2F;localhost&amp;#x2F;myapp_dev&amp;quot;
RAILS_ENV = &amp;quot;development&amp;quot;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;One file. Everything works.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-fuzzy-matching-that-actually-works&quot;&gt;The fuzzy matching that actually works&lt;&#x2F;h2&gt;
&lt;p&gt;This is where mise gets interesting. You don’t need exact command names.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ mise run test     # runs the test task
$ mise run tset     # still runs test (typo forgiven)
$ mise run tst      # yep, runs test
$ mise run deploy   # runs deploy task
$ mise run dply     # runs deploy
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The fuzzy matching is smart. It weighs:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Character position (earlier matches score higher)&lt;&#x2F;li&gt;
&lt;li&gt;Consecutive matches&lt;&#x2F;li&gt;
&lt;li&gt;Word boundaries&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;I tested this with 30+ tasks in one project. It still found the right one 90% of the time with 3-4 characters.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;tasks-live-where-they-should&quot;&gt;Tasks live where they should&lt;&#x2F;h2&gt;
&lt;p&gt;Instead of polluting the root with &lt;code&gt;scripts&#x2F;&lt;&#x2F;code&gt;, mise looks in &lt;code&gt;.mise&#x2F;tasks&#x2F;&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;.mise&amp;#x2F;
└── tasks&amp;#x2F;
    ├── db-reset.sh
    ├── cache-clear.sh
    └── logs-tail.sh
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Any executable in there becomes a task. No registration. No config.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ mise run db-reset    # runs .mise&amp;#x2F;tasks&amp;#x2F;db-reset.sh
$ mise run cache       # fuzzy matches to cache-clear.sh
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Shell scripts stay shell scripts. But now they’re discoverable:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ mise tasks
cache-clear   Clear all caches
db-reset      Reset database to clean state  
logs-tail     Tail production logs
test          Run all tests
deploy        Deploy to production
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;the-encryption-bit-that-matters&quot;&gt;The encryption bit that matters&lt;&#x2F;h2&gt;
&lt;p&gt;mise includes age encryption support. Not bolted on. Built in.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;toml&quot; class=&quot;language-toml &quot;&gt;&lt;code class=&quot;language-toml&quot; data-lang=&quot;toml&quot;&gt;# .mise.toml
[env]
DATABASE_URL = &amp;quot;postgresql:&amp;#x2F;&amp;#x2F;localhost&amp;#x2F;dev&amp;quot;
API_KEY = &amp;quot;age:SECRET_ENCRYPTED_STRING&amp;quot;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Set it up once:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ mise decrypt .mise.toml
Enter passphrase: 
$ export API_KEY=&amp;quot;actual-secret-key&amp;quot;
$ mise encrypt .mise.toml
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Your secrets are in the repo but encrypted. CI&#x2F;CD gets the age key. Developers get the age key. Random GitHub scrapers get nothing.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;integration-with-zed&quot;&gt;Integration with Zed&lt;&#x2F;h2&gt;
&lt;p&gt;This is where it gets smooth. In Zed, I set up task shortcuts:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;json&quot; class=&quot;language-json &quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&amp;#x2F;&amp;#x2F; .zed&amp;#x2F;tasks.json
{
  &amp;quot;tasks&amp;quot;: {
    &amp;quot;test&amp;quot;: {
      &amp;quot;command&amp;quot;: &amp;quot;mise run test&amp;quot;,
      &amp;quot;cwd&amp;quot;: &amp;quot;$WORKSPACE_ROOT&amp;quot;
    },
    &amp;quot;deploy&amp;quot;: {
      &amp;quot;command&amp;quot;: &amp;quot;mise run deploy&amp;quot;,
      &amp;quot;cwd&amp;quot;: &amp;quot;$WORKSPACE_ROOT&amp;quot;  
    }
  }
}
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now &lt;code&gt;cmd-shift-t&lt;&#x2F;code&gt; opens the task picker. Type “te”, hit enter. Tests run. The AI assistant sees the output inline. Fixes the code. Reruns the test. No context switching.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-breaks&quot;&gt;What breaks&lt;&#x2F;h2&gt;
&lt;p&gt;mise isn’t perfect. Here’s what I hit:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Windows support&lt;&#x2F;strong&gt;: Works through WSL. Native is rough.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Legacy tools&lt;&#x2F;strong&gt;: Some older ruby&#x2F;node versions don’t install clean. Same issue asdf has.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Task dependencies&lt;&#x2F;strong&gt;: Can’t do dynamic dependencies like make. Tasks depend on fixed task names.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Fuzzy matching confusion&lt;&#x2F;strong&gt;: With tasks named &lt;code&gt;deploy-staging&lt;&#x2F;code&gt; and &lt;code&gt;deploy-production&lt;&#x2F;code&gt;, typing &lt;code&gt;deploy&lt;&#x2F;code&gt; might pick wrong. Be specific or rename.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;the-tradeoffs&quot;&gt;The tradeoffs&lt;&#x2F;h2&gt;
&lt;p&gt;What I gained:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;One tool instead of 4 (asdf, direnv, make, scripts)&lt;&#x2F;li&gt;
&lt;li&gt;Fuzzy matching saves 100s of keystrokes daily&lt;&#x2F;li&gt;
&lt;li&gt;New devs get running in 2 commands: &lt;code&gt;mise install&lt;&#x2F;code&gt; and &lt;code&gt;mise run setup&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Secrets management that doesn’t suck&lt;&#x2F;li&gt;
&lt;li&gt;Task discovery that actually works&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;What I paid:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Another tool to install (but it replaces 4)&lt;&#x2F;li&gt;
&lt;li&gt;TOML syntax (not everyone’s favorite)&lt;&#x2F;li&gt;
&lt;li&gt;Rewriting Makefiles (took an afternoon per project)&lt;&#x2F;li&gt;
&lt;li&gt;Teaching the team new patterns (took a week)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;migration-pattern&quot;&gt;Migration pattern&lt;&#x2F;h2&gt;
&lt;p&gt;If you’re moving an existing project:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Install mise: &lt;code&gt;curl https:&#x2F;&#x2F;mise.jdx.dev&#x2F;install.sh | sh&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Import existing tool versions: &lt;code&gt;mise install&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Move one make target at a time to mise tasks&lt;&#x2F;li&gt;
&lt;li&gt;Move scripts to &lt;code&gt;.mise&#x2F;tasks&#x2F;&lt;&#x2F;code&gt; gradually&lt;&#x2F;li&gt;
&lt;li&gt;Add encryption last (less disruption)&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Start with the most-used tasks. Leave the weird legacy stuff in make until later.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-i-d-do-differently&quot;&gt;What I’d do differently&lt;&#x2F;h2&gt;
&lt;p&gt;After migrating 12 projects:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Start with &lt;code&gt;.mise.toml&lt;&#x2F;code&gt;, not &lt;code&gt;.mise&#x2F;config.yaml&lt;&#x2F;code&gt;. TOML is cleaner for this.&lt;&#x2F;li&gt;
&lt;li&gt;Put all tasks in &lt;code&gt;.mise&#x2F;tasks&#x2F;&lt;&#x2F;code&gt; as shell scripts first. Move to inline tasks only when needed.&lt;&#x2F;li&gt;
&lt;li&gt;Name tasks with clear prefixes: &lt;code&gt;db-reset&lt;&#x2F;code&gt;, &lt;code&gt;cache-clear&lt;&#x2F;code&gt;, &lt;code&gt;test-unit&lt;&#x2F;code&gt;. Makes fuzzy matching more predictable.&lt;&#x2F;li&gt;
&lt;li&gt;Document the age key setup immediately. People forget.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;the-bottom-line&quot;&gt;The bottom line&lt;&#x2F;h2&gt;
&lt;p&gt;mise replaced my entire project automation stack. The Rust rewrite isn’t just faster. It’s more thoughtful. Fuzzy matching, encrypted env vars, task discovery. These aren’t features. They’re fixes for real pain.&lt;&#x2F;p&gt;
&lt;p&gt;Every new project starts with &lt;code&gt;.mise.toml&lt;&#x2F;code&gt; now. Setup takes 5 minutes instead of an hour. New developers don’t message me asking how to run tests. They just run &lt;code&gt;mise tasks&lt;&#x2F;code&gt; and figure it out.&lt;&#x2F;p&gt;
&lt;p&gt;That’s the tool working.&lt;&#x2F;p&gt;
</description>
      </item>
      <item>
          <title>Disk space maintenance on Void Linux</title>
          <pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate>
          <author>Unknown</author>
          <link>https://raskell.io/articles/disk-space-void-linux-maintenance/</link>
          <guid>https://raskell.io/articles/disk-space-void-linux-maintenance/</guid>
          <description xml:base="https://raskell.io/articles/disk-space-void-linux-maintenance/">&lt;h2 id=&quot;monday-morning-surprise&quot;&gt;Monday morning surprise&lt;&#x2F;h2&gt;
&lt;p&gt;As I spent most time doing stuff with my computer rather than configuring my beloved Linux distribution, Void Linux, I have developed the tendency to not really bother about Void at all until something crucial becomes unusable. After almost two years of having switched from Arch to Void, I have actually never encountered any major problem and felt I had made the right decision.&lt;&#x2F;p&gt;
&lt;p&gt;I checked my disk usage out of curiosity if the 250GB solid-state disk would be enough. And there came the surprise:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;shell&quot; class=&quot;language-shell &quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;$ df -H
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        8.4G     0  8.4G   0% &amp;#x2F;dev
tmpfs           8.4G  1.9M  8.4G   1% &amp;#x2F;dev&amp;#x2F;shm
tmpfs           8.4G  1.4M  8.4G   1% &amp;#x2F;run
&amp;#x2F;dev&amp;#x2F;nvme0n1p3  138G  117G   21G  85% &amp;#x2F;
efivarfs        158k   85k   69k  56% &amp;#x2F;sys&amp;#x2F;firmware&amp;#x2F;efi&amp;#x2F;efivars
cgroup          8.4G     0  8.4G   0% &amp;#x2F;sys&amp;#x2F;fs&amp;#x2F;cgroup
&amp;#x2F;dev&amp;#x2F;nvme0n1p4  366G   34G  332G  10% &amp;#x2F;home
&amp;#x2F;dev&amp;#x2F;nvme0n1p1  536M  152k  536M   1% &amp;#x2F;boot&amp;#x2F;efi
tmpfs           8.4G   25k  8.4G   1% &amp;#x2F;tmp
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;My root partition was full, way too full in my opinion. Did I miss something? Is Void not what I was looking for after all? I don’t enjoy baby sitting my OS &lt;em&gt;du jour&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-painless-solution&quot;&gt;The painless solution&lt;&#x2F;h2&gt;
&lt;p&gt;After a quick Brave search, I ended up finding what I was looking for. Some kind fellow software engineer from China didn’t shy away to make a blog post about his journey when he faced the very same problem. Out of annoyance of having to deal with that, I copy-pasted as quickly as possible, not minding what kind of side-effects I might run into, these three commands.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;1-cleaning-the-package-cache&quot;&gt;1. Cleaning the package cache&lt;&#x2F;h3&gt;
&lt;p&gt;All the knowledge I was lacking was to be found with the man page of &lt;code&gt;xbps-remove&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;shell&quot; class=&quot;language-shell &quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;# xbps-remove -yO
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;man.voidlinux.org&#x2F;xbps-remove.1#O,&quot;&gt;man page&lt;&#x2F;a&gt; of &lt;code&gt;xbps-remove&lt;&#x2F;code&gt; tells us the &lt;code&gt;-O&lt;&#x2F;code&gt; parameter takes care of &lt;em&gt;cleaning the cache directory removing obsolete binary packages.&lt;&#x2F;em&gt; Obsolete binary packages? Good riddance! I was surprised this to learn that solely this step freed up almost half of my used root partition disk space.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-removing-orphaned-packages&quot;&gt;2. Removing orphaned packages&lt;&#x2F;h3&gt;
&lt;pre data-lang=&quot;shell&quot; class=&quot;language-shell &quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;# xbps-remove -yo
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Here the same &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;man.voidlinux.org&#x2F;xbps-remove.1#o,&quot;&gt;man page&lt;&#x2F;a&gt; tells us that the &lt;code&gt;-o&lt;&#x2F;code&gt; parameter takes care of &lt;em&gt;removing installed package orphans that were installed automatically (as dependencies) and are not currently dependencies of any installed package.&lt;&#x2F;em&gt; As before, good riddance!&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-purging-old-unused-kernels&quot;&gt;3. Purging old, unused kernels&lt;&#x2F;h3&gt;
&lt;p&gt;This one is interesting. While I knew about the circumstance that the people behind Void had developed their own package management ecosystem, I hadn’t fully realized there were other utilities that came along with the upstream Void installation which were there for me to manage my beloved OS. So, apparently, one of these is a &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;void-linux&#x2F;void-packages&#x2F;blob&#x2F;master&#x2F;srcpkgs&#x2F;base-files&#x2F;files&#x2F;vkpurge&quot;&gt;shell script&lt;&#x2F;a&gt; name &lt;code&gt;vkpurge&lt;&#x2F;code&gt;, I must assume as a short name for &lt;code&gt;Void&#x27;s Kernel purging&lt;&#x2F;code&gt; tool. I like this type of naming heavily implying its functionality.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;shell&quot; class=&quot;language-shell &quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;# vkpurge rm all
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;It performed as expected. Old kernel files (and modules?) were indeed purged and freed up even more disk space. I should add that this step is optional as it is always useful to have some old kernels at hand when things hit the fan (which for me, they haven’t in a very, very long time).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;result&quot;&gt;Result&lt;&#x2F;h2&gt;
&lt;p&gt;I couldn’t be happier.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;shell&quot; class=&quot;language-shell &quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;$ df -H
Filesystem      Size  Used Avail Use% Mounted on
...
&amp;#x2F;dev&amp;#x2F;nvme0n1p3  138G   45G   93G  33% &amp;#x2F;
...
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;renewal-of-faith&quot;&gt;Renewal of faith&lt;&#x2F;h2&gt;
&lt;p&gt;Overall, why am I even writing this if some other fellow engineer already figured this out? Simply, because I would therefore be able to explain why I have enjoyed my journey with Void as my go-to Linux distribution. It keeps things simple. Some well-documented utilities. As simple that a simple Brave search suffices to find the answer to my problems.&lt;&#x2F;p&gt;
&lt;p&gt;This very aspect of Void is worthwhile highlighing. I remember more arcane Linux distributions that had me in their grip in figuring things out. Many Googles searches were necessary and even more trial and errors attempts to get simple things fixed.&lt;&#x2F;p&gt;
&lt;p&gt;Now back to my Monday morning.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;div class=&quot;footnote-definition&quot; id=&quot;1&quot;&gt;&lt;sup class=&quot;footnote-definition-label&quot;&gt;1&lt;&#x2F;sup&gt;
&lt;p&gt;Painting in header image is “Seaside” by Aleksandr Deyneka&lt;&#x2F;p&gt;
&lt;&#x2F;div&gt;
</description>
      </item>
      <item>
          <title>All beginning is Haskell</title>
          <pubDate>Mon, 06 Mar 2023 00:00:00 +0000</pubDate>
          <author>Unknown</author>
          <link>https://raskell.io/articles/all-beginning-is-haskell/</link>
          <guid>https://raskell.io/articles/all-beginning-is-haskell/</guid>
          <description xml:base="https://raskell.io/articles/all-beginning-is-haskell/">&lt;h2 id=&quot;the-beauty-of-mathematics&quot;&gt;The beauty of mathematics&lt;&#x2F;h2&gt;
&lt;p&gt;Mathematics is often considered beautiful because it provides a way to describe and understand the world in a way that is precise, elegant, and abstract. Mathematics can be seen as a language that allows us to describe complex phenomena and patterns in a way that is independent of the particular objects or situations involved. This abstract quality of mathematics is what makes it so powerful and beautiful.&lt;&#x2F;p&gt;
&lt;p&gt;Mathematics can also be considered beautiful because of its ability to reveal unexpected connections and relationships between seemingly disparate areas of study. By using the language of mathematics to describe complex phenomena, we are able to identify deep and fundamental connections between seemingly unrelated areas of study, revealing the underlying structure and order in the natural world.&lt;&#x2F;p&gt;
&lt;p&gt;As someone who always was drawn by this type of mathematical beauty, I also was naturally drawn to Haskell due to its roots in mathematical theory since the first time I read about Haskell. Haskell was created by a group of researchers who were interested in creating a language that was as pure as possible. The language was designed to be purely functional, meaning that all computations are performed through the evaluation of functions. This approach allows Haskell programs to be more concise and easier to reason about than programs written in imperative languages like Java or C++. As a language that emphasizes purity and immutability, Haskell is particularly well-suited to applications in mathematics, data analysis, and other areas where correctness and maintainability are critical.&lt;&#x2F;p&gt;
&lt;p&gt;In addition to its mathematical roots, Haskell’s focus on purity and immutability also appealed to me as a programmer. As someone who has worked on projects with large codebases, I understand the importance of maintainability and the challenges that can arise when code is difficult to reason about or modify. By focusing on pure functions and immutability, Haskell reduces the potential for bugs and makes it easier to modify code without introducing new problems.&lt;&#x2F;p&gt;
&lt;p&gt;In this blog post, I will provide an introduction to Haskell and explore how its focus on pure functions and easy refactoring can make code maintenance easier. I’ll also provide some interesting code snippets to help illustrate Haskell’s unique approach to programming. Whether you’re a seasoned developer or a math enthusiast like myself, I hope that this post will give you a deeper appreciation for the beauty and power of Haskell.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;mister-curry-haskell&quot;&gt;Mister Curry Haskell&lt;&#x2F;h2&gt;
&lt;p&gt;Haskell is named after the logician Haskell Curry, who was a pioneer in the field of mathematical logic and contributed significantly to the development of the lambda calculus. The lambda calculus is a mathematical notation system for expressing computation that is closely related to functional programming.&lt;&#x2F;p&gt;
&lt;p&gt;Haskell was developed in the late 1980s by a group of researchers, including Simon Peyton Jones, Philip Wadler, and others, who were interested in creating a purely functional programming language that was inspired by the lambda calculus. They named the language after Haskell Curry as a tribute to his contributions to mathematical logic.&lt;&#x2F;p&gt;
&lt;p&gt;Haskell’s focus on purity and immutability is closely related to the lambda calculus, which is based on the idea of functions as first-class citizens. In Haskell, functions are treated as values that can be passed around, composed, and evaluated just like any other data type. This approach allows Haskell programs to be more concise and easier to reason about than programs written in imperative languages like Java or C++.&lt;&#x2F;p&gt;
&lt;p&gt;Haskell also incorporates many features from other functional programming languages like ML and Lisp, as well as concepts from category theory and other branches of mathematics. The language’s sophisticated type system, which includes type inference, type classes, and higher-kinded types, is one of its most distinctive and powerful features.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-lambda-is-the-new-alpha&quot;&gt;The Lambda is the new Alpha&lt;&#x2F;h2&gt;
&lt;p&gt;Haskell is closely related to the lambda calculus, which is a mathematical notation system for expressing computation. The lambda calculus was developed in the early 20th century by mathematician Alonzo Church as a way to formalize the notion of computability.&lt;&#x2F;p&gt;
&lt;p&gt;The lambda calculus consists of three basic elements: variables, functions, and applications. Variables represent values, functions represent computations, and applications represent the act of applying a function to an argument. For example, the function $ f(x) = x + 1 $ could be represented in the lambda calculus as the expression $ (λx.x + 1) $ .&lt;&#x2F;p&gt;
&lt;p&gt;Haskell was developed in the late 1980s by a group of researchers, including Simon Peyton Jones, Philip Wadler, and others, who were interested in creating a purely functional programming language that was inspired by the lambda calculus. As mentioned above, they named the language after Haskell Curry as a tribute to his contributions to mathematical logic.&lt;&#x2F;p&gt;
&lt;p&gt;Like the lambda calculus, Haskell is based on the principles of functional programming, which emphasize the use of pure functions and immutable data structures. In Haskell, functions are first-class citizens that can be passed around and composed just like any other data type. This allows Haskell programmers to create powerful abstractions and write code that is more modular and reusable than in imperative languages like C++ or Java.&lt;&#x2F;p&gt;
&lt;p&gt;Haskell’s type system is also inspired by the lambda calculus, and includes features like type inference, type classes, and higher-kinded types. This allows Haskell programmers to write code that is both statically typed and expressive, reducing the potential for errors while still allowing for elegant and concise code.&lt;&#x2F;p&gt;
&lt;p&gt;Here’s an example of a lambda calculus expression that represents the addition of two numbers:&lt;&#x2F;p&gt;
&lt;p&gt;$$ (λx.λy.x + y) $$&lt;&#x2F;p&gt;
&lt;p&gt;In this expression, λx represents a function that takes an input x, and λy represents a function that takes an input y. The expression x + y represents the sum of x and y.&lt;&#x2F;p&gt;
&lt;p&gt;To use this expression to add two numbers, we apply it to two arguments:&lt;&#x2F;p&gt;
&lt;p&gt;$$ (λx.λy.x + y) 3 4 $$&lt;&#x2F;p&gt;
&lt;p&gt;In this case, the expression is applied to the arguments 3 and 4, resulting in the following computation:&lt;&#x2F;p&gt;
&lt;p&gt;$$ (λy.3 + y) 4 $$&lt;&#x2F;p&gt;
&lt;p&gt;In this expression, the function λx has been replaced with the value 3, resulting in a new function that takes an input y and adds it to 3. This function is then applied to the argument 4, resulting in the value 7.&lt;&#x2F;p&gt;
&lt;p&gt;This example demonstrates the basic principles of the lambda calculus, where functions are used to represent computations and can be composed and applied in a way that is similar to arithmetic operations. These principles form the basis of functional programming, and have been influential in the development of languages like Haskell, which was designed to be closely aligned with the lambda calculus.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;one-type-system-to-rule-them-all&quot;&gt;One Type System to rule them all&lt;&#x2F;h2&gt;
&lt;p&gt;ML (Meta Language) is considered a precursor to Haskell because it is a functional programming language that shares many of the same concepts and ideas. Like Haskell, ML is strongly typed and supports type inference, pattern matching, and higher-order functions. ML was developed in the 1970s by Robin Milner and others as a tool for writing software that could reason about mathematical objects and proofs. It was initially designed as a metalanguage for describing programming languages, but quickly evolved into a full-fledged programming language in its own right.&lt;&#x2F;p&gt;
&lt;p&gt;One of the key features of ML that influenced the development of Haskell is its support for algebraic data types. Algebraic data types allow programmers to define complex data structures by combining simpler types in various ways, using constructs like sums, products, and variants. This allows for a high degree of expressiveness and modularity in programming. Haskell also inherits many other features from ML, such as its focus on purity, immutability, and higher-order functions. ML also introduced the concept of type inference, which allows type information to be inferred automatically from the code, reducing the need for explicit type annotations.&lt;&#x2F;p&gt;
&lt;p&gt;Haskell’s type system is based on the Hindley-Milner type system, which was first introduced in the ML programming language in the 1970s. The Hindley-Milner type system is a type inference algorithm that can deduce the types of expressions in a program without the need for explicit type annotations. In Haskell, the Hindley-Milner type system has been extended and refined to include additional features such as type classes, higher-kinded types, and generalized algebraic data types. These extensions make Haskell’s type system more expressive and flexible than the original Hindley-Milner type system in ML.&lt;&#x2F;p&gt;
&lt;p&gt;Type classes are a key feature of Haskell’s type system that allow programmers to define common behavior for a set of types. For example, the &lt;code&gt;Eq&lt;&#x2F;code&gt; type class defines the notion of equality for a given type, while the Ord type class defines the notion of order. Type classes provide a powerful mechanism for creating modular and reusable code.&lt;&#x2F;p&gt;
&lt;p&gt;Higher-kinded types are another extension to Haskell’s type system that allow for more complex and abstract types. In essence, higher-kinded types are types that take other types as arguments, allowing for a greater degree of generality and abstraction in programming.&lt;&#x2F;p&gt;
&lt;p&gt;Generalized algebraic data types (GADTs) are a further refinement of algebraic data types that allow for more precise control over the types of data in a program. GADTs allow programmers to specify more specific constraints on the types of data that can be manipulated, leading to more robust and maintainable code.&lt;&#x2F;p&gt;
&lt;p&gt;Haskell’s type system builds on the foundations laid by the Hindley-Milner type system in ML, but extends and refines it to include additional features that make Haskell’s type system more expressive and flexible. By providing a powerful and expressive type system, Haskell makes it easier for programmers to write correct and maintainable code.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;eli5-what-is-haskell-again&quot;&gt;ELI5: What is Haskell again?&lt;&#x2F;h2&gt;
&lt;p&gt;Haskell is a functional programming language that was first developed in the late 1980s. It is named after the logician Haskell Curry and is designed to be purely functional, meaning that all computations are performed through the evaluation of functions. This approach allows Haskell programs to be more concise and easier to reason about than programs written in imperative languages like Java or C++.&lt;&#x2F;p&gt;
&lt;p&gt;Haskell is a statically-typed language, which means that the type of every expression is known at compile time. This allows the compiler to catch many errors before the program is even run, making it easier to write correct code. Haskell also has a sophisticated type system that allows for powerful abstractions and code reuse.&lt;&#x2F;p&gt;
&lt;p&gt;One of the most distinctive features of Haskell is its laziness. This means that expressions are only evaluated when they are needed, allowing for efficient use of resources and the creation of infinite data structures.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;getting-started-with-haskell&quot;&gt;Getting started with Haskell&lt;&#x2F;h2&gt;
&lt;p&gt;To get started with Haskell, you’ll need to install the GHC compiler and an editor or IDE that supports Haskell development. Once you have these tools set up, you can start writing Haskell code!&lt;&#x2F;p&gt;
&lt;p&gt;Let’s start with a simple “Hello, world!” program:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;haskell&quot; class=&quot;language-haskell &quot;&gt;&lt;code class=&quot;language-haskell&quot; data-lang=&quot;haskell&quot;&gt;main :: IO ()
main = putStrLn &amp;quot;Hello, world!&amp;quot;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This program defines a main function that prints the string “Hello, world!” to the console. The :: operator is used to specify the type of the main function, which in this case is IO (). The IO type represents actions that interact with the outside world, like reading from or writing to files.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;from-pure-functions-to-easy-refactoring&quot;&gt;From pure functions to easy refactoring&lt;&#x2F;h2&gt;
&lt;p&gt;One of the key benefits of Haskell’s functional programming paradigm is that it makes it easy to write pure functions. A pure function is one that has no side effects and always returns the same result given the same input. This property makes pure functions easy to reason about and test.&lt;&#x2F;p&gt;
&lt;p&gt;Let’s look at an example. Suppose we have a function that calculates the factorial of a number:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;haskell&quot; class=&quot;language-haskell &quot;&gt;&lt;code class=&quot;language-haskell&quot; data-lang=&quot;haskell&quot;&gt;factorial :: Integer -&amp;gt; Integer
factorial n = product [1..n]
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This function takes an Integer and returns its factorial. We can use it like this:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;haskell&quot; class=&quot;language-haskell &quot;&gt;&lt;code class=&quot;language-haskell&quot; data-lang=&quot;haskell&quot;&gt;&amp;gt; factorial 5
120
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now suppose we want to optimize this function by memoizing the results. We can do this easily by using Haskell’s memoize function:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;haskell&quot; class=&quot;language-haskell &quot;&gt;&lt;code class=&quot;language-haskell&quot; data-lang=&quot;haskell&quot;&gt;import Data.Function.Memoize (memoize)

factorial :: Integer -&amp;gt; Integer
factorial = memoize go
  where
    go 0 = 1
    go n = n * factorial (n - 1)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The memoize function takes a function and returns a memoized version of it. This means that the function’s results are cached for future calls with the same arguments, improving performance.&lt;&#x2F;p&gt;
&lt;p&gt;Note that we didn’t need to change any of the code that calls the factorial function. This is because the function’s type signature didn’t change, and its behavior is still the same. This is an example of how easy refactoring can be in Haskell. By focusing on pure functions and immutability, Haskell makes it easy to modify code without introducing bugs or breaking existing functionality.&lt;&#x2F;p&gt;
&lt;p&gt;The absence of side effects and the use of pure functions can greatly aid in reasoning about code. By eliminating side effects and enforcing purity, it becomes much easier to reason about the behavior of a program and to understand its logic.&lt;&#x2F;p&gt;
&lt;p&gt;Here are some ways in which pure functions and the absence of side effects help with reasoning about code:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Predictability: Since pure functions do not have side effects, their behavior is entirely determined by their inputs. This makes it easy to predict the output of a pure function given a particular input, making it easier to reason about the behavior of the program as a whole.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Testability: Pure functions are easy to test because their behavior is entirely determined by their inputs. This makes it easy to write automated tests for pure functions, ensuring that they behave correctly and that any changes to their behavior can be caught early on.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Modularity: Pure functions are highly modular because they can be composed and reused in a variety of ways. Since they do not have side effects, they can be combined and used in a wide range of contexts without introducing unexpected behavior or side effects.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Reasoning about state: Pure functions make it easy to reason about the state of a program because they do not modify state. This makes it easier to understand the flow of data through a program and to reason about the relationships between different parts of the program.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The absence of side effects and the use of pure functions can greatly aid in reasoning about code by making it more predictable, testable, modular, and easier to understand. By enforcing purity and avoiding side effects, functional programming languages like Haskell can make it easier to reason about the behavior of complex programs, leading to more robust and maintainable code.&lt;&#x2F;p&gt;
&lt;p&gt;The use of pure functions and the absence of side effects can greatly improve the refactoring potential and code maintainability of a program.&lt;&#x2F;p&gt;
&lt;p&gt;Refactoring is the process of changing the structure of a program without changing its behavior. This can involve adding new features, improving performance, or making the code more modular and reusable. By using pure functions and avoiding side effects, it becomes much easier to refactor a program without introducing unexpected behavior or breaking existing functionality.&lt;&#x2F;p&gt;
&lt;p&gt;By using pure functions and avoiding side effects, functional programming languages like Haskell make it easier to refactor code without introducing bugs or unintended behavior. This improves code maintainability by making it easier to modify and update the codebase over time, reducing the risk of introducing new bugs or breaking existing functionality.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;purity-as-a-design-goal&quot;&gt;Purity as a design goal&lt;&#x2F;h2&gt;
&lt;p&gt;Pure functions are functions that have no side effects and always return the same result given the same input. In other words, a pure function’s output only depends on its input, and it doesn’t modify any external state or perform any I&#x2F;O operations.&lt;&#x2F;p&gt;
&lt;p&gt;This property is desirable for several reasons. First, pure functions are easier to reason about and test. Because they have no side effects, they always produce the same output given the same input, which makes them predictable and easier to verify. This can be especially important in mathematical or scientific contexts where correctness is critical.&lt;&#x2F;p&gt;
&lt;p&gt;Second, pure functions can be composed more easily than impure functions. Because pure functions have no side effects, they can be combined or nested in any order without affecting the final result. This makes it easier to write modular and reusable code that can be composed from smaller building blocks.&lt;&#x2F;p&gt;
&lt;p&gt;Third, pure functions can be more efficient than impure functions in certain contexts. Because pure functions are referentially transparent, meaning that their results depend only on their inputs, they can be memoized or cached to improve performance. This can be especially useful in recursive algorithms or in situations where expensive computations need to be repeated many times.&lt;&#x2F;p&gt;
&lt;p&gt;The concept of pure functions is closely related to mathematical theory, particularly the notion of functions in mathematical analysis.&lt;&#x2F;p&gt;
&lt;p&gt;In mathematics, a function is a mapping between two sets, where each element of the first set is associated with a unique element of the second set. For example, the function f(x) = x^2 is a mapping from the set of real numbers to the set of non-negative real numbers. Given any real number x, the function f(x) returns the square of that number.&lt;&#x2F;p&gt;
&lt;p&gt;Functions in mathematics are often defined in terms of their inputs and outputs, without reference to any external state or context. This is similar to the concept of pure functions in programming, which have no side effects and depend only on their input parameters.&lt;&#x2F;p&gt;
&lt;p&gt;The concept of referential transparency, which is central to the idea of pure functions in programming, also has a mathematical counterpart. In mathematical analysis, a function is said to be referentially transparent if it can be replaced by its value without changing the outcome of any other computations. For example, the function f(x) = x + 1 is referentially transparent, because it can be replaced by its value without affecting the outcome of any other computations.&lt;&#x2F;p&gt;
&lt;p&gt;The use of pure functions in programming can be seen as an extension of these mathematical concepts to the realm of software engineering. By designing programs in terms of pure functions, we can create software systems that are easier to reason about and test, more modular and reusable, and more efficient in certain contexts. This can lead to more robust and maintainable software systems that are better suited to the demands of modern computing.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-side-effects-of-functional-purity&quot;&gt;The side-effects of functional purity&lt;&#x2F;h2&gt;
&lt;p&gt;The concepts of immutability, statelessness, higher-order functions, and lazy evaluation are closely related to the idea of pure functions in programming and are highly prominent in the advantages of a programming language like Haskell. By embracing these concepts, we can create software systems that are more robust, efficient, and maintainable.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Immutability&lt;&#x2F;strong&gt;: Immutability is the concept of not modifying data once it has been created. Immutable data structures are often used in functional programming, because they can make it easier to reason about the behavior of a program. In Haskell, many of the built-in data structures are immutable, including lists, tuples, and sets.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Statelessness&lt;&#x2F;strong&gt;: Statelessness refers to the concept of not maintaining any internal state between function calls. In other words, a stateless function depends only on its input parameters and has no side effects. Stateless functions are often used in functional programming, because they can make it easier to reason about the behavior of a program.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Higher-order functions&lt;&#x2F;strong&gt;: Higher-order functions are functions that take other functions as input parameters or return functions as output. Higher-order functions are often used in functional programming to create powerful abstractions and compose smaller functions into larger ones.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Lazy evaluation&lt;&#x2F;strong&gt;: Lazy evaluation is the concept of delaying the evaluation of an expression until it is actually needed. In Haskell, expressions are evaluated only when their values are required to compute the final result of a program. This can lead to more efficient code, because expressions that are not needed are never evaluated.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;All of these concepts are closely related to pure functions, because they help enforce the properties of purity and immutability. By using immutable data structures and stateless functions, we can reduce the potential for side effects and create code that is more predictable and easier to reason about. Higher-order functions and lazy evaluation can also lead to more efficient and maintainable code by creating powerful abstractions and reducing the amount of unnecessary computation.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;nomad-as-monad&quot;&gt;Nomad as monad&lt;&#x2F;h2&gt;
&lt;p&gt;In a purely functional language like Haskell, functions that produce side effects like input&#x2F;output, state mutation, and exceptions are generally not considered pure. This is because they have the potential to produce different results for the same input, depending on the context in which they are executed.&lt;&#x2F;p&gt;
&lt;p&gt;To work with these effects in a purely functional way, Haskell uses monads to encapsulate the side effects and provide a way to compose and sequence them in a predictable way. For example, the IO monad is used to represent input&#x2F;output operations, while the State monad is used to represent stateful computations.&lt;&#x2F;p&gt;
&lt;p&gt;In the case of the IO monad, the monadic structure allows the input&#x2F;output operations to be performed in a predictable and composable way. The IO monad provides functions like &amp;gt;&amp;gt;= (pronounced “bind”) and &amp;gt;&amp;gt; (pronounced “then”) that allow input&#x2F;output operations to be sequenced and composed in a way that ensures their order of execution is well-defined.&lt;&#x2F;p&gt;
&lt;p&gt;Similarly, the State monad provides a way to model stateful computations in a pure way. The State monad allows a function to compute a new state value based on an existing state value, without modifying the existing state value. This allows the state to be propagated through a series of function calls in a predictable and composable way.&lt;&#x2F;p&gt;
&lt;p&gt;Monads are therefore a fundamental concept in Haskell and are used to encapsulate effects, such as input&#x2F;output or state changes, in a purely functional way. Let’s take a look at a simple example that demonstrates the use of a monad in Haskell:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;haskell&quot; class=&quot;language-haskell &quot;&gt;&lt;code class=&quot;language-haskell&quot; data-lang=&quot;haskell&quot;&gt;import Control.Monad.State

-- Define a function that uses the State monad to keep track of a counter
count :: State Int Int
count = do
  n &amp;lt;- get
  put (n + 1)
  return n

-- Use the count function to increment a counter and retrieve its value
result :: Int
result = evalState count 0
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;In this example, we use the State monad to keep track of a counter. The State monad encapsulates stateful computations in a purely functional way, allowing us to write code that has side effects while still adhering to Haskell’s purity and immutability.&lt;&#x2F;p&gt;
&lt;p&gt;The count function uses the State monad to read the current value of the counter, increment it by 1, and return its original value. This is achieved using the get, put, and return functions provided by the State monad.&lt;&#x2F;p&gt;
&lt;p&gt;The result variable uses the evalState function to run the count function with an initial state of 0. This results in the counter being incremented by 1 and its value being returned, which is then assigned to the result variable.&lt;&#x2F;p&gt;
&lt;p&gt;This example demonstrates how monads can be used to encapsulate side effects in a purely functional way, making it easier to reason about and maintain code. The State monad is just one example of a monad in Haskell, and there are many others, such as the IO monad for input&#x2F;output operations and the Maybe monad for handling optional values.&lt;&#x2F;p&gt;
&lt;p&gt;By using monads, Haskell programmers can write code that has side effects in a way that is consistent with the language’s focus on purity and immutability. This can lead to more robust and maintainable software systems that are easier to test and modify over time.&lt;&#x2F;p&gt;
&lt;p&gt;In mathematics, a monad is a structure that describes a certain kind of algebraic operation. Specifically, a monad is a triple (T, η, μ) consisting of:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;A functor T that maps a category C to itself. This functor is often called the “monad functor”.&lt;&#x2F;li&gt;
&lt;li&gt;A natural transformation η from the identity functor on C to T. This natural transformation is called the “unit” of the monad.&lt;&#x2F;li&gt;
&lt;li&gt;A natural transformation μ from T∘T to T. This natural transformation is called the “multiplication” of the monad.
The monad structure is used in mathematics to describe a wide range of mathematical structures and operations, including algebraic structures like groups, rings, and fields, as well as more abstract structures like topological spaces and sheaves.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;In computer science, monads are used as a programming construct that provides a way to encapsulate computation and control side effects in a purely functional way. In this context, a monad is typically defined as a type constructor that provides a way to combine and sequence operations in a way that is both composable and predictable. Monads are often used in functional programming languages like Haskell to model a wide range of effects, including input&#x2F;output, state, and exceptions.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;why-everyone-should-learn-a-haskell-for-a-great-good&quot;&gt;Why everyone should learn a Haskell for a great good&lt;&#x2F;h2&gt;
&lt;p&gt;One of the benefits of learning Haskell is that it can help you develop a deeper understanding of programming concepts like recursion, higher-order functions, and lazy evaluation. Haskell’s focus on purity and immutability can also help you develop good programming habits that can be applied to other languages.&lt;&#x2F;p&gt;
&lt;p&gt;However, despite its many advantages, Haskell is not as commonly used as other programming languages like Java or Python. This is due in part to its steep learning curve and the fact that it is less widely taught in universities and other educational institutions. Haskell also has a smaller community of developers than some other languages, which can make it harder to find resources and support.&lt;&#x2F;p&gt;
&lt;p&gt;Despite these challenges, there are many reasons to learn and use Haskell. For example, Haskell’s focus on purity and immutability can lead to more robust and maintainable code. The language’s type system can also catch many errors at compile time, reducing the potential for bugs. In addition, Haskell’s focus on functional programming can help you develop good programming habits that can be applied to other languages. Here are some of the reasons why it is worthwhile to learn Haskell:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Learning Haskell can help you develop a deeper understanding of programming concepts like recursion, higher-order functions, and lazy evaluation. Haskell’s focus on purity and immutability can also help you develop good programming habits that can be applied to other languages.&lt;&#x2F;li&gt;
&lt;li&gt;Haskell’s sophisticated type system can catch many errors at compile time, reducing the potential for bugs. This can lead to more robust and maintainable code.&lt;&#x2F;li&gt;
&lt;li&gt;Haskell’s focus on purity and immutability can lead to more elegant and concise code that is easier to reason about and test.&lt;&#x2F;li&gt;
&lt;li&gt;Haskell is a powerful tool for data analysis, machine learning, and other computational tasks that involve complex algorithms and mathematical models.&lt;&#x2F;li&gt;
&lt;li&gt;Haskell has a small but passionate community of developers who are constantly pushing the boundaries of what is possible with functional programming.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;&#x2F;h2&gt;
&lt;p&gt;One of the key benefits of Haskell’s functional programming paradigm is that it makes it easy to write maintainable code. By focusing on pure functions and immutability, Haskell reduces the potential for bugs and makes it easier to modify code without introducing new problems.&lt;&#x2F;p&gt;
&lt;p&gt;The use of pure functions means that code is easier to reason about and test. Pure functions have no side effects and always return the same result given the same input. This property makes them easy to test and reduces the potential for bugs. In addition, pure functions are composable, meaning that they can be combined to create more complex functions. This allows for the creation of highly modular and reusable code, which can simplify the process of maintaining and updating code over time.&lt;&#x2F;p&gt;
&lt;p&gt;Haskell’s emphasis on immutability also contributes to code maintainability. Immutable data structures are less prone to bugs and can be easier to reason about than mutable ones. Immutable data structures also encourage a functional style of programming, which can lead to more concise and elegant code.&lt;&#x2F;p&gt;
&lt;p&gt;Ultimately, Haskell’s focus on functional programming, immutability, and easy refactoring makes it an ideal choice for building maintainable code. By reducing the potential for bugs and making it easier to modify code over time, Haskell can help developers create software that is robust and easy to maintain.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;references-and-further-reading&quot;&gt;References and further reading&lt;&#x2F;h2&gt;
</description>
      </item>
      <item>
          <title>My OpenBSD journey: Getting it virtualized with libvirt (1)</title>
          <pubDate>Mon, 06 Feb 2023 00:00:00 +0000</pubDate>
          <author>Unknown</author>
          <link>https://raskell.io/articles/my-openbsd-journey-getting-it-virtualized-with-libvirt-1/</link>
          <guid>https://raskell.io/articles/my-openbsd-journey-getting-it-virtualized-with-libvirt-1/</guid>
          <description xml:base="https://raskell.io/articles/my-openbsd-journey-getting-it-virtualized-with-libvirt-1/">&lt;h2 id=&quot;void-linux-as-my-daily-driver&quot;&gt;Void Linux as my daily driver&lt;&#x2F;h2&gt;
&lt;p&gt;Around six months ago, I decided to ditch my long in the tooth Arch-based setup on my belovest Thinkpad X1 Carbon. I’ve been very loyal over the years, and almost came to belive that Arch will be a constant in my adult life. While I kept up with upcoming technologies, I somehow lost track of the ever so diversifying landscape of Linux distributions. It took me a while of constantly coming across a generically named reference to what seemed to be yet another Linux distribution. That outwardly generic sounding name, &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;voidlinux.org&#x2F;&quot;&gt;Void Linux&lt;&#x2F;a&gt;, kept poking my curiosity by supposedly feeling like Arch Linux in the old days, while sharing some substantial DNA with the BSD operating systems. Yet, that’s another story I might tell another day, but to remain brief, the BSDs, in particular the infamous OpenBSD with its quite infamous lead developer Theo De Raadt, always were what I considered the endgame. The holy grail of Unix operating systems, so did I think over the decades, FreeBSD, NetBSD and OpenBSD, have always been on my personal radar and I felt I had to earn the intellectual capacity to be able to properly put them at use one day. Last year, when I made the (almost painless) switch from Arch Linux to Void Linux, the simplicity and especially the barebone experience of Void reignited the fascination and the admiration I always had for the BSD operating systems and their philosophy.&lt;&#x2F;p&gt;
&lt;p&gt;While I could write (and definitely will in the near future) about how my journey onto Void Linux and how it has been so far, I preferred to write down, in some like diary, and document every step on how one can approach and ultimately use &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.openbsd.org&#x2F;&quot;&gt;OpenBSD&lt;&#x2F;a&gt; in 2023. Big disclaimer, I’ve yet to install OpenBSD on some baremetal server I ordered some days ago, but dabbled around in the meantime with OS-level virtualization in order to get it running. That’s what brings me to &lt;em&gt;libvirt&lt;&#x2F;em&gt; and my surprise to learn that I wouldn’t need some full-fledged virtualization solution like the ones offered by VirtualBox or VMWare to efficiently run a virtualized OpenBSD machine. So, ok, let’s recap, so far we got the following bill of materials:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Void Linux as the host system&lt;&#x2F;li&gt;
&lt;li&gt;OpenBSD to be virtualized on that host system&lt;&#x2F;li&gt;
&lt;li&gt;libvirt as the glue that makes virtualization feel like black magic&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;from-void-to-openbsd&quot;&gt;From Void to OpenBSD&lt;&#x2F;h3&gt;
&lt;p&gt;Before I tell you more about the history of how things went down while setting up OpenBSD, let me give you some basic notions about both Void Linux, being one out of many Linux distributions for the sake of simplicity representing them all as it ended up being my distribution of choice, and OpenBSD. As already mentioned earlier, Void Linux and OpenBSD are both Unix-like operating systems, but they feature enough differences to make them noteworthy. Here are a few similarities and differences between the two:&lt;&#x2F;p&gt;
&lt;p&gt;What is similar:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Both are free and open-source operating systems.&lt;&#x2F;li&gt;
&lt;li&gt;Both use a package manager for software management. Void Linux uses XBPS, while OpenBSD uses pkg_add.&lt;&#x2F;li&gt;
&lt;li&gt;Both prioritize security and stability in their development and design.&lt;&#x2F;li&gt;
&lt;li&gt;Both feature a version control based package repository, meaning that changes in build definition are managed by pull requests from users.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;What is different:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;License: Void Linux is licensed under the MIT License, while OpenBSD is licensed under the ISC License.&lt;&#x2F;li&gt;
&lt;li&gt;Philosophy: OpenBSD prioritizes security and privacy, while Void Linux prioritizes simplicity and modularity.&lt;&#x2F;li&gt;
&lt;li&gt;Package Management: Void Linux uses a binary package manager (XBPS), while OpenBSD uses a source-based package manager (pkg_add).&lt;&#x2F;li&gt;
&lt;li&gt;Package Repository: Void Linux has a large and diverse repository, while OpenBSD has a smaller and more curated repository.&lt;&#x2F;li&gt;
&lt;li&gt;Init System: Void Linux uses runit as its init system, while OpenBSD uses rc. None uses the infamous systemd init system.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;what-is-openbsd-in-a-nutshell&quot;&gt;What is OpenBSD in a nutshell&lt;&#x2F;h2&gt;
&lt;p&gt;OpenBSD is a free and open-source operating system that focuses on security, standardization, and robustness. It is based on the Berkeley Software Distribution (BSD) Unix operating system and is developed by a global community of volunteers. OpenBSD aims to provide a secure platform for both personal and enterprise use by implementing strong security features, including access control mechanisms, encryption, and auditing.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Theo_de_Raadt&quot;&gt;Theo de Raadt&lt;&#x2F;a&gt; is the founder and lead developer of OpenBSD. His main objective with OpenBSD is to create a secure operating system that is free from backdoors, vulnerabilities, and other security weaknesses. He is committed to auditing the source code of the operating system and third-party software included with it, to identify and remove any potential security risks. De Raadt is also dedicated to improving the overall quality of the codebase and ensuring compatibility with a wide range of hardware and software.&lt;&#x2F;p&gt;
&lt;p&gt;What makes OpenBSD really special and stand out is that is developed a suite of tools that got adopted by other OSs like Linux, macOS or even Windows. One of the most famous instances of such adoption is the now de facto standard openssh suite. It actually emerged from within the development circle of the OpenBSD project. OpenBSD also implemented a wide range of OS features that are by now considered staples among other OSs, things like Linux-based OS-level containerization done via the means of cgroups, something that OpenBSD already pioneered and solved with a different spin many years before Linux with pledge and unveil. Go check out &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;why-openbsd.rocks&#x2F;fact&#x2F;freezero&#x2F;&quot;&gt;Why OpenBSD rocks&lt;&#x2F;a&gt; to get a feel what makes OpenBSD so unique and interesting.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;virtualization-with-libvirt&quot;&gt;Virtualization with libvirt&lt;&#x2F;h3&gt;
&lt;p&gt;So now, let’s get back to our virtualization endeavour where we would like to virtualize OpenBSD on a Void Linux installation. If you happen to be using another Linux distribution, most of the individual steps would be very similar. That brings me to the next technology we should explain a bit more here, and that is libvirt.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;libvirt.org&#x2F;&quot;&gt;libvirt&lt;&#x2F;a&gt; is an open-source virtualization management library that provides a simple and unified API for managing virtualization technologies, including KVM, QEMU, Xen, and others. It aims to simplify the process of creating, managing, and migrating virtual machines, storage, and networks, and to make it easier for administrators to manage virtual environments.&lt;&#x2F;p&gt;
&lt;p&gt;To virtualize an operating system like OpenBSD with libvirt, you need to follow these steps:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Install libvirt and the virtualization technology you want to use, such as KVM.&lt;&#x2F;li&gt;
&lt;li&gt;Download the OpenBSD iso file and place it in a location accessible by libvirt.&lt;&#x2F;li&gt;
&lt;li&gt;Create a new virtual machine in libvirt with the OpenBSD ISO as the installation media. This can be done through the command line or using a graphical user interface such as virt-manager.&lt;&#x2F;li&gt;
&lt;li&gt;Configure the virtual machine, including the amount of memory, CPU, and disk space, to meet the requirements of OpenBSD.&lt;&#x2F;li&gt;
&lt;li&gt;Start the virtual machine and install OpenBSD as you would on a physical machine.&lt;&#x2F;li&gt;
&lt;li&gt;Once the installation is complete, you can configure the virtual network, storage, and other settings as required.&lt;&#x2F;li&gt;
&lt;li&gt;Finally, you can use the libvirt API or the command line to manage and control the virtual machine, including starting, stopping, migrating, and snapshotting.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h3 id=&quot;step-by-step-guide&quot;&gt;Step-by-step guide&lt;&#x2F;h3&gt;
&lt;p&gt;Let’s first install the &lt;code&gt;libvirt&lt;&#x2F;code&gt; package and some related packages which we need in order to connect via VNC. The VNC will provide us with the possibility to use the graphical interface of the running OpenBSD instance.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ sudo xbps-install -S dbus qemu libvirt virt-manager virt-viewer tigervnc
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now, we need to add our user, in my case &lt;code&gt;raskell&lt;&#x2F;code&gt;, to the &lt;code&gt;libvirt&lt;&#x2F;code&gt; group which got simultanously created with the installation of libvirt.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ sudo usermod -aG libvirt raskell
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;OpenBSD features a release cycle of six months. We would need to update our system every six month to keep up with the latest packages. During a given release, only security and bug fix patches are applied to the curated packages maintained by pkg_add. Therefore, in February 2023, we’re using the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.openbsd.org&#x2F;72.html&quot;&gt;OpenBSD 7.2&lt;&#x2F;a&gt; release version. As I’m living in Switzerland, I chose to pull the iso image from a Swiss mirror, in this case from &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;mirror.ungleich.ch&#x2F;pub&#x2F;OpenBSD&#x2F;7.2&#x2F;&quot;&gt;&lt;code&gt;mirror.ungleich.ch&#x2F;pub&#x2F;OpenBSD&lt;&#x2F;code&gt;&lt;&#x2F;a&gt; (check what mirror is closest to you to get the best download rate).&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;# cd &amp;#x2F;var&amp;#x2F;lib&amp;#x2F;libvirt&amp;#x2F;boot&amp;#x2F;
# sudo wget https:&amp;#x2F;&amp;#x2F;mirror.ungleich.ch&amp;#x2F;pub&amp;#x2F;OpenBSD&amp;#x2F;7.2&amp;#x2F;amd64&amp;#x2F;install72.iso
--2023-01-12 20:48:15--  https:&amp;#x2F;&amp;#x2F;mirror.ungleich.ch&amp;#x2F;pub&amp;#x2F;OpenBSD&amp;#x2F;7.2&amp;#x2F;amd64&amp;#x2F;install72.iso
Resolving mirror.ungleich.ch (mirror.ungleich.ch)... 2a0a:e5c0:2:2:400:c8ff:fe68:bef3, 185.203.114.135
Connecting to mirror.ungleich.ch (mirror.ungleich.ch)|2a0a:e5c0:2:2:400:c8ff:fe68:bef3|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 583352320 (556M) [application&amp;#x2F;octet-stream]
Saving to: ‘install72.iso.1’

install72.iso.1                      100%[====================================================================&amp;gt;] 556.33M  7.11MB&amp;#x2F;s    in 77s     

2023-01-12 20:49:33 (7.19 MB&amp;#x2F;s) - ‘install72.iso.1’ saved [583352320&amp;#x2F;583352320]
sudo wget https:&amp;#x2F;&amp;#x2F;mirror.ungleich.ch&amp;#x2F;pub&amp;#x2F;OpenBSD&amp;#x2F;7.2&amp;#x2F;amd64&amp;#x2F;SHA256
--2023-01-12 20:47:38--  https:&amp;#x2F;&amp;#x2F;mirror.ungleich.ch&amp;#x2F;pub&amp;#x2F;OpenBSD&amp;#x2F;7.2&amp;#x2F;amd64&amp;#x2F;SHA256
Resolving mirror.ungleich.ch (mirror.ungleich.ch)... 2a0a:e5c0:2:2:400:c8ff:fe68:bef3, 185.203.114.135
Connecting to mirror.ungleich.ch (mirror.ungleich.ch)|2a0a:e5c0:2:2:400:c8ff:fe68:bef3|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1992 (1.9K) [application&amp;#x2F;octet-stream]
Saving to: ‘SHA256.1’

SHA256.1                             100%[====================================================================&amp;gt;]   1.95K  --.-KB&amp;#x2F;s    in 0s      

2023-01-12 20:47:39 (742 MB&amp;#x2F;s) - ‘SHA256.1’ saved [1992&amp;#x2F;1992]
# grep install63.iso SHA256 &amp;gt; &amp;#x2F;tmp&amp;#x2F;x
# sha256sum -c &amp;#x2F;tmp&amp;#x2F;x
# rm &amp;#x2F;tmp&amp;#x2F;x
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Before we can start the virtualization server and get running our OpenBSD instance, we need to define the configuraiton on how to virtualize and ultimately boot the system with. This is done with &lt;code&gt;virt-install&lt;&#x2F;code&gt;. Noteworthy here is that we &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.qemu.org&#x2F;&quot;&gt;QEMU&lt;&#x2F;a&gt; as our emulation solution of choice, we allocate up to 4GB of RAM and 4 CPU cores to the machine.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ sudo virt-install \
      --name=openbsd \
      --virt-type=qemu \
      --memory=2048,maxmemory=4096 \
      --vcpus=2,maxvcpus=4 \
      --cpu host \
      --os-variant=openbsd7.0 \
      --cdrom=&amp;#x2F;var&amp;#x2F;lib&amp;#x2F;libvirt&amp;#x2F;boot&amp;#x2F;install72.iso \
      --network=bridge=virbr0,model=virtio \
      --graphics=vnc \
      --disk path=&amp;#x2F;var&amp;#x2F;lib&amp;#x2F;libvirt&amp;#x2F;images&amp;#x2F;openbsd.qcow2,size=40,bus=virtio,format=qcow2
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Once, it is up and running, we can use a vnc solution to connect to the running machine. In this case, I chose &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.libvirt.org&#x2F;manpages&#x2F;virsh.html&quot;&gt;virsh&lt;&#x2F;a&gt; to do the job. virsh is a command-line interface tool for managing virtualization environments created with libvirt. It allows us to manage virtual machines, storage pools, and network interfaces, as well as other virtualization components, from the command line.&lt;&#x2F;p&gt;
&lt;p&gt;To establish a VNC connection with a running libvirt virtualized OpenBSD instance, you can use the following steps:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Start the virtual machine in libvirt: You can start the virtual machine using the virsh command &lt;strong&gt;&lt;code&gt;virsh start &amp;lt;vm-name&amp;gt;&lt;&#x2F;code&gt;&lt;&#x2F;strong&gt;, where &lt;strong&gt;&lt;code&gt;&amp;lt;vm-name&amp;gt;&lt;&#x2F;code&gt;&lt;&#x2F;strong&gt; is the name of the virtual machine you want to start.&lt;&#x2F;li&gt;
&lt;li&gt;Find the VNC display: Once the virtual machine is running, you can find the VNC display number for the virtual machine using the virsh command &lt;strong&gt;&lt;code&gt;virsh vncdisplay &amp;lt;vm-name&amp;gt;&lt;&#x2F;code&gt;&lt;&#x2F;strong&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;Connect to the VNC display: You can connect to the VNC display using a VNC client, such as &lt;strong&gt;&lt;code&gt;vncviewer&lt;&#x2F;code&gt;&lt;&#x2F;strong&gt;, and specify the IP address of the host running the virtual machine and the VNC display number. For example, if the host’s IP address is &lt;strong&gt;&lt;code&gt;192.168.0.100&lt;&#x2F;code&gt;&lt;&#x2F;strong&gt; and the VNC display number is &lt;strong&gt;&lt;code&gt;:0&lt;&#x2F;code&gt;&lt;&#x2F;strong&gt;, the command to connect would be &lt;strong&gt;&lt;code&gt;vncviewer 192.168.0.100:0&lt;&#x2F;code&gt;&lt;&#x2F;strong&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;Authenticate to the VNC server: You may need to enter a password to authenticate to the VNC server. The password is set when the virtual machine is created in libvirt.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;With these steps, you can establish a VNC connection with a running libvirt virtualized OpenBSD instance and interact with the virtual machine’s graphical user interface.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ virsh dumpxml openbsd | grep vnc
&amp;lt;graphics type=&amp;#x27;vnc&amp;#x27; port=&amp;#x27;5900&amp;#x27; autoport=&amp;#x27;yes&amp;#x27; listen=&amp;#x27;127.0.0.1&amp;#x27;&amp;gt;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Like I did, most of you would like to interact with a graphical interface such as with X11. For that, we yet another tool, a so-called VNC viewer. A very simple implementation of such a vnc viewer is &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;tigervnc.org&#x2F;&quot;&gt;tigervnc&lt;&#x2F;a&gt; (simply install it with &lt;code&gt;$ sudo xbps-install -S tigervnc&lt;&#x2F;code&gt;).&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ sudo virsh --connect
qemu:&amp;#x2F;&amp;#x2F;&amp;#x2F;system start openbsd
Domain &amp;#x27;openbsd&amp;#x27; started
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;{tbc}&lt;&#x2F;p&gt;
&lt;h2 id=&quot;references-and-further-reading&quot;&gt;References and further reading&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;voidlinux&#x2F;comments&#x2F;ghwvv5&#x2F;guide_how_to_setup_qemukvm_emulation_on_void_linux&#x2F;&quot;&gt;[Guide] how to setup QEMU&#x2F;KVM emulation on void linux | &#x2F;r&#x2F;voidlinux&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.cyberciti.biz&#x2F;faq&#x2F;kvmvirtualization-virt-install-openbsd-unix-guest&#x2F;&quot;&gt;KVM virt-install: Install OpenBSD As Guest Operating System&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.skreutz.com&#x2F;posts&#x2F;autoinstall-openbsd-on-qemu&#x2F;&quot;&gt;Auto-install OpenBSD on QEMU | skreutz.com&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</description>
      </item>
      <item>
          <title>Hello and outlook</title>
          <pubDate>Tue, 20 Sep 2022 00:00:00 +0000</pubDate>
          <author>Unknown</author>
          <link>https://raskell.io/articles/hello-and-outlook/</link>
          <guid>https://raskell.io/articles/hello-and-outlook/</guid>
          <description xml:base="https://raskell.io/articles/hello-and-outlook/">&lt;h2 id=&quot;hello-world&quot;&gt;Hello world&lt;&#x2F;h2&gt;
&lt;p&gt;Welcome to my tech blog! My name is Raffael and I am excited to share my rumings about the state of tech, open-source, and the life as a software developer with you.&lt;&#x2F;p&gt;
&lt;p&gt;In this first post, I wanted to introduce myself and explain what you can expect to find on this blog. I have been working in the tech industry for several years, and I have experience with a variety of technologies, programming languages, and operating systems. I have a strong interest in open-source software and the principles of open collaboration and sharing that underpin it. Also, I have a passion for tech hardware and the funtional restoration of discarded equipment. The freedom to express oneself with technology to make the world a more comfortable place is a strong focus of mine.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;outlook&quot;&gt;Outlook&lt;&#x2F;h2&gt;
&lt;p&gt;On this blog, I will be covering a wide range of topics related to technology in general, open-source, operating systems, programming, and tech hardware. Some of the things you can expect to find include:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;My dabblings into new and bleeding-edge technologies in general&lt;&#x2F;li&gt;
&lt;li&gt;My experience in using Linux, BSDs, and embedded operating systems&lt;&#x2F;li&gt;
&lt;li&gt;Exploration of new and exciting open-source projects&lt;&#x2F;li&gt;
&lt;li&gt;Discussion of the latest technology trends and innovations&lt;&#x2F;li&gt;
&lt;li&gt;Personal musings and insights into the tech industry&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;I believe that technology has the power to change the world, and I am excited to be a small part of that change. I hope that this blog will serve as a valuable resource for anyone interested in technology, open-source, Linux, OpenBSD, and programming. I will be posting new content on a regular basis, so be sure to check back often.&lt;&#x2F;p&gt;
&lt;p&gt;In my next post, I will dive into a specific topic and share my knowledge and experience. I want to make sure that my readers will learn something new every time they visit my blog.&lt;&#x2F;p&gt;
&lt;p&gt;Thank you for visiting, and I look forward to connecting with you on social media or in real life.&lt;&#x2F;p&gt;
</description>
      </item>
    </channel>
</rss>
